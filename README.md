# Recognizing the Unseen: A Multimodal, Trauma-Informed AI Framework for Crisis Detection and Clinical Assessment

> Graduate independent study exploring interpretable AI for trauma-aware emotional state recognition and clinical augmentation.

---

## ðŸ“Œ Project Summary

This research explores a trauma-informed, multimodal AI framework that detects affective patterns such as emotional shutdown, dissociation, or depressive flat affect in high-stakes environments. The goal is to supportâ€”not replaceâ€”human decision-making in clinical and crisis response contexts.

This first phase leverages the **DAIC-WOZ dataset** to prototype an emotionally-aware, ethically guided ML system with a strong emphasis on transparency and real-world clinical relevance.

---

## ðŸ§  Why It Matters

Traditional AI systems often misinterpret trauma responsesâ€”mistaking withdrawal, flat affect, or dissociation for resistance or noncompliance. This work contributes to the growing movement toward **compassion-centered**, **bias-mitigated**, and **interpretable** machine learning solutions.

---

## ðŸ“š Dataset Used

| Dataset     | Source    | Status              | Notes                                                       |
|-------------|-----------|---------------------|-------------------------------------------------------------|
| DAIC-WOZ    | USC-ICT   | âœ… Approved/downloaded | Depression & anxiety interviews with rich multimodal cues   |

---

## ðŸ› ï¸ Tools & Libraries

- Python 3.12
- OpenFace, OpenSMILE for facial and audio features
- HuggingFace Transformers (BERT / DistilBERT)
- SHAP or LIME for explainability
- Jupyter Notebook
- `.venv` (virtual environment)

---

## ðŸ§ª Technical Focus

- **Modalities**: Facial cues, vocal tone, and linguistic sentiment
- **Modeling**: Multimodal fusion + affective state classification
- **Explainability**: SHAP/LIME outputs with clinical interpretability
- **Ethics**: Transparent bias handling + human-centered model framing

---

## ðŸ“¦ Project Structure

- trauma-informed-ai-framework/
  - data/ â€” Preprocessed DAIC-WOZ data
  - notebooks/ â€” Exploratory modeling & EDA
  - models/ â€” Saved model artifacts
  - utils/ â€” Custom feature extraction scripts
  - README.md â€” You're here
  - thesis_scope_vision.md â€” Research statement and project vision

---

## ðŸ”­ Upcoming Milestones

- [ ] Complete full EDA on DAIC-WOZ  
- [ ] Prototype initial multimodal classification model  
- [ ] Integrate SHAP explainability outputs  
- [ ] Draft publication-ready article  
- [ ] Prepare visuals and dashboard for final submission/presentation  

---

## ðŸ§¾ License

This work is covered under the [MIT License](https://opensource.org/licenses/MIT).

---

For questions, collaborations, or media inquiries, contact:  
**Michelle (Elle) Lynn George**  
[Michelle.L.George@vanderbilt.edu](mailto:Michelle.L.George@vanderbilt.edu)  
[https://ellelynn.netlify.app](https://ellelynn.netlify.app)

---

> âœ¨ This project is part of an independent study at Vanderbilt University, with the goal of contributing a publishable, reproducible, and human-centered AI model to support trauma-informed care.

