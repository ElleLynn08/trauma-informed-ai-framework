{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 07 Microexpression Modeling Kickoff ‚Äî CASME II + SMIC Fusion  \n",
    "Last polished on: 2025-10-14\n",
    "\n",
    "### Purpose\n",
    "This notebook launches the first emotion modeling phase of the trauma-informed AI framework.  \n",
    "Using the fused metadata from CASME II and SMIC, we will:\n",
    "\n",
    "- Frame the difference between **micro** and **macro** expressions\n",
    "- Engineer features based on **duration, modality, and action units**\n",
    "- Train early classifiers to **predict emotion labels**\n",
    "- Prepare for downstream Z3 symbolic verification (Notebook 08)\n",
    "\n",
    "---\n",
    "\n",
    "### Input:\n",
    "- `fused_microexpression_metadata.parquet` (from Notebook 06)\n",
    "\n",
    "### Output:\n",
    "- Classifier artifacts (joblib / pickle)\n",
    "- Cleaned modeling data\n",
    "- Visuals: confusion matrix, ROC, emotion distributions\n",
    "\n",
    "---\n",
    "\n",
    "### Reminder:\n",
    "All saves must go to:\n",
    "- `outputs/checks/` ‚Üí for `.parquet`, `.csv`, `.joblib`\n",
    "- `outputs/visuals/` ‚Üí for plots and diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.0 Microexpression Modeling Kickoff \n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Begin emotion modeling using fused CASME II + SMIC metadata\n",
    "#   - Engineer emotion features, temporal windows, and AU tags\n",
    "#   - Build early exploratory models (baseline classifiers, timelines, flags)\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Define root paths (project-level consistency) ----------------------------\n",
    "ROOT = Path.cwd().parent  # From /notebooks/, go up to project root\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CHECKS_DIR = ROOT / \"outputs\" / \"checks\"\n",
    "VIS_DIR = ROOT / \"outputs\" / \"visuals\"\n",
    "\n",
    "# --- Create output folders if missing -----------------------------------------\n",
    "CHECKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Confirm notebook init ----------------------------------------------------\n",
    "print(\"‚úÖ Notebook 07 initialized successfully\")\n",
    "print(f\"üìÇ Root:       {ROOT}\")\n",
    "print(f\"üìÇ Checks:     {CHECKS_DIR}\")\n",
    "print(f\"üìÇ Visuals:    {VIS_DIR}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7.1 Load Fused Metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load the cleaned metadata that includes both CASME II and SMIC microexpression records.\n",
    "# This will be the foundation for all modeling and AU-based augmentation.\n",
    "# =============================================================================\n",
    "\n",
    "FUSED_PATH = CHECKS_DIR / \"fused_microexpression_metadata.parquet\"\n",
    "\n",
    "try:\n",
    "    fusion_df = pd.read_parquet(FUSED_PATH)\n",
    "    print(f\"‚úÖ Loaded fused metadata: {fusion_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Fused metadata not found at: {FUSED_PATH}\")\n",
    "    fusion_df = None\n",
    "\n",
    "# --- Preview structure and distribution ---------------------------------------\n",
    "if fusion_df is not None:\n",
    "    display(fusion_df.head(3))\n",
    "    display(fusion_df.info())\n",
    "    print(\"‚úÖ Emotion distribution:\")\n",
    "    print(fusion_df[\"Emotion\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.2 Engineer Microexpression Features\n",
    "\n",
    "This step sets the stage for any model to learn patterns by creating useful, numeric features from raw metadata.\n",
    "\n",
    " Goals:\n",
    "\n",
    "- Convert Onset, Peak, Offset, and Duration to numeric\n",
    "\n",
    "- Compute Latency (time between Onset and Peak)\n",
    "\n",
    "- Compute Intensity Window (Peak to Offset)\n",
    "\n",
    "- Count ActionUnits (AU count from string list like \"4+L10\" ‚Üí 2)\n",
    "\n",
    "- Normalize casing in Modality, handle missing values if needed\n",
    "\n",
    "- Confirm feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2 Engineer Microexpression Features\n",
    "# -----------------------------------------------------------------------------\n",
    "# Convert timing columns to numeric and compute derived features:\n",
    "#   - Latency = Peak - Onset\n",
    "#   - Intensity = Offset - Peak\n",
    "#   - AU_Count = number of Action Units (e.g., \"4+L10\" ‚Üí 2)\n",
    "# Also standardize modality and handle missing values.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Convert columns to numeric ------------------------------------------------\n",
    "cols_to_numeric = [\"Onset\", \"Peak\", \"Offset\", \"Duration\"]\n",
    "for col in cols_to_numeric:\n",
    "    fusion_df[col] = pd.to_numeric(fusion_df[col], errors=\"coerce\")\n",
    "\n",
    "# --- Derive latency and intensity ---------------------------------------------\n",
    "fusion_df[\"Latency\"] = fusion_df[\"Peak\"] - fusion_df[\"Onset\"]\n",
    "fusion_df[\"Intensity\"] = fusion_df[\"Offset\"] - fusion_df[\"Peak\"]\n",
    "\n",
    "# --- Count Action Units -------------------------------------------------------\n",
    "# Handles values like \"4+L10\", \"12\", etc.\n",
    "def count_aus(entry):\n",
    "    if pd.isna(entry):\n",
    "        return 0\n",
    "    return len(str(entry).split(\"+\"))\n",
    "\n",
    "fusion_df[\"AU_Count\"] = fusion_df[\"ActionUnits\"].apply(count_aus)\n",
    "\n",
    "# --- Normalize modality casing ------------------------------------------------\n",
    "fusion_df[\"Modality\"] = fusion_df[\"Modality\"].str.upper()\n",
    "\n",
    "# --- Check nulls and structure ------------------------------------------------\n",
    "display(fusion_df[[\"Onset\", \"Peak\", \"Offset\", \"Latency\", \"Intensity\", \"AU_Count\"]].describe())\n",
    "print(\"‚úÖ Feature engineering complete ‚Äî ready for modeling!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2.1 Save Feature-Engineered Microexpression Metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "# Purpose:\n",
    "#   - Save the updated DataFrame after computing Latency, Intensity, AU_Count\n",
    "#   - Stored as safe .parquet format for reuse in 7.3 modeling pipeline\n",
    "# =============================================================================\n",
    "\n",
    "FEATURES_PATH = CHECKS_DIR / \"microexpression_features.parquet\"\n",
    "\n",
    "fusion_df.to_parquet(FEATURES_PATH, index=False)\n",
    "print(f\"‚úÖ Saved engineered features ‚Üí {FEATURES_PATH.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üï∑Ô∏è SPider Check- Confirm save worked -----------------------------------------------------\n",
    "if FEATURES_PATH.exists():\n",
    "    print(\"üìÇ Feature file contents:\")\n",
    "    display(pd.read_parquet(FEATURES_PATH).sample(3))\n",
    "else:\n",
    "    print(\"‚ùå Save failed ‚Äî file not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.3 Microexpression Emotion Modeling Kickoff\n",
    "Purpose:\n",
    "   Build baseline classifiers to predict emotion labels using facial-action \n",
    "   metadata features (Latency, Intensity, AU_Count).\n",
    "   Evaluate model performance with accuracy, F1-score, and confusion matrix.\n",
    "   Save predictions for integration with Z3 rule logic in Notebook 08.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3 Microexpression Emotion Modeling Kickoff\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train baseline classifiers (LogReg, RF, KNN) on facial metadata.\n",
    "# Use a pipeline with imputation + scaling.\n",
    "# Evaluate with classification metrics and confusion matrices.\n",
    "# Save Z3-ready predictions as a separate cell to ensure image output finalizes.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Import Packages ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Load Features -----------------------------------------------------------\n",
    "ROOT = Path.cwd().parent\n",
    "FEATURE_PATH = ROOT / \"outputs\" / \"checks\" / \"microexpression_features.parquet\"\n",
    "df = pd.read_parquet(FEATURE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loaded features: {df.shape}\")\n",
    "display(df.head())\n",
    "\n",
    "# --- Visualize Emotion Distribution ------------------------------------------\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=\"Emotion\", data=df, order=df[\"Emotion\"].value_counts().index)\n",
    "plt.title(\"Emotion Class Distribution\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Train/Test Split --------------------------------------------------------\n",
    "X = df[[\"Latency\", \"Intensity\", \"AU_Count\"]]\n",
    "y = df[\"Emotion\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trauma_ai)",
   "language": "python",
   "name": "trauma_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
