{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 07 Microexpression Modeling ‚Äî CASME II + SMIC Fusion  \n",
    "### Project: Trauma-Informed AI Framework  \n",
    "### Author: Michelle Lynn George (Elle)  \n",
    "### Institution: Vanderbilt University, School of Engineering  \n",
    "### Year: 2025  \n",
    "### Version: 1.0  \n",
    "### Date of last run: 2025-11-24\n",
    "### Last polished on: 2025-10-15\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "This notebook launches the first emotion modeling phase of the trauma-informed AI framework.  \n",
    "Using the fused metadata from CASME II and SMIC, we will:\n",
    "\n",
    "- Frame the difference between **micro** and **macro** expressions\n",
    "- Engineer features based on **duration, modality, and action units**\n",
    "- Train early classifiers to **predict emotion labels**\n",
    "- Prepare for downstream Z3 symbolic verification (Notebook 08)\n",
    "\n",
    "---\n",
    "\n",
    "### Input:\n",
    "- `fused_microexpression_metadata.parquet` (from Notebook 06)\n",
    "\n",
    "### Output:\n",
    "- Classifier artifacts (joblib / pickle)\n",
    "- Cleaned modeling data\n",
    "- Visuals: confusion matrix, ROC, emotion distributions\n",
    "\n",
    "---\n",
    "\n",
    "### Reminder:\n",
    "All saves must go to:\n",
    "- `outputs/checks/` ‚Üí for `.parquet`, `.csv`, `.joblib`\n",
    "- `outputs/visuals/` ‚Üí for plots and diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.0 Microexpression Modeling Kickoff \n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Begin emotion modeling using fused CASME II + SMIC metadata\n",
    "#   - Engineer emotion features, temporal windows, and AU tags\n",
    "#   - Build early exploratory models (baseline classifiers, timelines, flags)\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Define root paths (project-level consistency) ----------------------------\n",
    "ROOT = Path.cwd().parent  # From /notebooks/, go up to project root\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CHECKS_DIR = ROOT / \"outputs\" / \"checks\"\n",
    "VIS_DIR = ROOT / \"outputs\" / \"visuals\"\n",
    "\n",
    "# --- Create output folders if missing -----------------------------------------\n",
    "CHECKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Confirm notebook init ----------------------------------------------------\n",
    "print(\"‚úÖ Notebook 07 initialized successfully\")\n",
    "print(f\"üìÇ Root:       {ROOT}\")\n",
    "print(f\"üìÇ Checks:     {CHECKS_DIR}\")\n",
    "print(f\"üìÇ Visuals:    {VIS_DIR}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7.1 Load Fused Metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load the cleaned metadata that includes both CASME II and SMIC microexpression records.\n",
    "# This will be the foundation for all modeling and AU-based augmentation.\n",
    "# =============================================================================\n",
    "\n",
    "FUSED_PATH = CHECKS_DIR / \"fused_microexpression_metadata.parquet\"\n",
    "\n",
    "try:\n",
    "    fusion_df = pd.read_parquet(FUSED_PATH)\n",
    "    print(f\"‚úÖ Loaded fused metadata: {fusion_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Fused metadata not found at: {FUSED_PATH}\")\n",
    "    fusion_df = None\n",
    "\n",
    "# --- Preview structure and distribution ---------------------------------------\n",
    "if fusion_df is not None:\n",
    "    display(fusion_df.head(3))\n",
    "    display(fusion_df.info())\n",
    "    print(\"‚úÖ Emotion distribution:\")\n",
    "    print(fusion_df[\"Emotion\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üï∑Ô∏è Spider Check - verify SMIC & CASME2 are both in the intended fused dataset\n",
    "#  üß†‚ú® data integrity triumph!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üî¢ Distribution of samples by source dataset:\")\n",
    "print(fusion_df[\"SourceDataset\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.2 Engineer Microexpression Features\n",
    "\n",
    "This step sets the stage for any model to learn patterns by creating useful, numeric features from raw metadata.\n",
    "\n",
    " Goals:\n",
    "\n",
    "- Convert Onset, Peak, Offset, and Duration to numeric\n",
    "\n",
    "- Compute Latency (time between Onset and Peak)\n",
    "\n",
    "- Compute Intensity Window (Peak to Offset)\n",
    "\n",
    "- Count ActionUnits (AU count from string list like \"4+L10\" ‚Üí 2)\n",
    "\n",
    "- Normalize casing in Modality, handle missing values if needed\n",
    "\n",
    "- Confirm feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2 Engineer Microexpression Features\n",
    "# -----------------------------------------------------------------------------\n",
    "# Convert timing columns to numeric and compute derived features:\n",
    "#   - Latency = Peak - Onset\n",
    "#   - Intensity = Offset - Peak\n",
    "#   - AU_Count = number of Action Units (e.g., \"4+L10\" ‚Üí 2)\n",
    "# Also standardize modality and handle missing values.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Convert columns to numeric ------------------------------------------------\n",
    "cols_to_numeric = [\"Onset\", \"Peak\", \"Offset\", \"Duration\"]\n",
    "for col in cols_to_numeric:\n",
    "    fusion_df[col] = pd.to_numeric(fusion_df[col], errors=\"coerce\")\n",
    "\n",
    "# --- Derive latency and intensity ---------------------------------------------\n",
    "fusion_df[\"Latency\"] = fusion_df[\"Peak\"] - fusion_df[\"Onset\"]\n",
    "fusion_df[\"Intensity\"] = fusion_df[\"Offset\"] - fusion_df[\"Peak\"]\n",
    "\n",
    "# --- Count Action Units -------------------------------------------------------\n",
    "# Handles values like \"4+L10\", \"12\", etc.\n",
    "def count_aus(entry):\n",
    "    if pd.isna(entry):\n",
    "        return 0\n",
    "    return len(str(entry).split(\"+\"))\n",
    "\n",
    "fusion_df[\"AU_Count\"] = fusion_df[\"ActionUnits\"].apply(count_aus)\n",
    "\n",
    "# --- Normalize modality casing ------------------------------------------------\n",
    "fusion_df[\"Modality\"] = fusion_df[\"Modality\"].str.upper()\n",
    "\n",
    "# --- Check nulls and structure ------------------------------------------------\n",
    "display(fusion_df[[\"Onset\", \"Peak\", \"Offset\", \"Latency\", \"Intensity\", \"AU_Count\"]].describe())\n",
    "print(\"‚úÖ Feature engineering complete ‚Äî ready for modeling!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2.1 Save Feature-Engineered Microexpression Metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "# Purpose:\n",
    "#   - Save the updated DataFrame after computing Latency, Intensity, AU_Count\n",
    "#   - Stored as safe .parquet format for reuse in 7.3 modeling pipeline\n",
    "# =============================================================================\n",
    "\n",
    "FEATURES_PATH = CHECKS_DIR / \"microexpression_features.parquet\"\n",
    "\n",
    "fusion_df.to_parquet(FEATURES_PATH, index=False)\n",
    "print(f\"‚úÖ Saved engineered features ‚Üí {FEATURES_PATH.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üï∑Ô∏è SPider Check- Confirm save worked -----------------------------------------------------\n",
    "if FEATURES_PATH.exists():\n",
    "    print(\"üìÇ Feature file contents:\")\n",
    "    display(pd.read_parquet(FEATURES_PATH).sample(3))\n",
    "else:\n",
    "    print(\"‚ùå Save failed ‚Äî file not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.3 Microexpression Emotion Modeling Kickoff\n",
    "Purpose:\n",
    "   Build baseline classifiers to predict emotion labels using facial-action \n",
    "   metadata features (Latency, Intensity, AU_Count).\n",
    "   Evaluate model performance with accuracy, F1-score, and confusion matrix.\n",
    "   Save predictions for integration with Z3 rule logic in Notebook 08.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3 Microexpression Emotion Modeling Kickoff\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train baseline classifiers (LogReg, RF, KNN) on facial metadata.\n",
    "# Use a pipeline with imputation + scaling.\n",
    "# Evaluate with classification metrics and confusion matrices.\n",
    "# Save Z3-ready predictions as a separate cell to ensure image output finalizes.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Import Packages ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Load Features -----------------------------------------------------------\n",
    "ROOT = Path.cwd().parent\n",
    "FEATURE_PATH = ROOT / \"outputs\" / \"checks\" / \"microexpression_features.parquet\"\n",
    "df = pd.read_parquet(FEATURE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loaded features: {df.shape}\")\n",
    "display(df.head())\n",
    "\n",
    "# --- Visualize Emotion Distribution ------------------------------------------\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=\"Emotion\", data=df, order=df[\"Emotion\"].value_counts().index)\n",
    "plt.title(\"Emotion Class Distribution\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Train/Test Split --------------------------------------------------------\n",
    "X = df[[\"Latency\", \"Intensity\", \"AU_Count\"]]\n",
    "y = df[\"Emotion\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.1 Load Microexpression Features from Notebook 06\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "FEATURE_PATH = ROOT / \"outputs\" / \"checks\" / \"microexpression_features.parquet\"\n",
    "df = pd.read_parquet(FEATURE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loaded features: {df.shape}\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.2  Train/Test Split ‚Äî Microexpression Classifier Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell prepares X and y features from engineered microexpression metadata.\n",
    "# It splits data into training and test sets for model evaluation.\n",
    "# Target = Emotion class. Features = Latency, Intensity, AU_Count\n",
    "# =============================================================================\n",
    "\n",
    "# Define input and output columns\n",
    "X = df[[\"Latency\", \"Intensity\", \"AU_Count\"]]\n",
    "y = df[\"Emotion\"]\n",
    "\n",
    "# Split into train and test sets (80/20 stratified)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {X_train.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.3  Train & Evaluate ‚Äî Baseline Classifiers (LogReg, RF, KNN)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell trains three baseline classifiers on microexpression features:\n",
    "#   - Logistic Regression\n",
    "#   - Random Forest\n",
    "#   - K-Nearest Neighbors\n",
    "#\n",
    "# Each model uses a pipeline with:\n",
    "#   - Median imputation for missing values\n",
    "#   - Standard scaling of features\n",
    "#   - A classifier specified in the model loop\n",
    "#\n",
    "# Evaluation includes:\n",
    "#   - Accuracy and macro-averaged F1 score\n",
    "#   - Confusion matrix visualization\n",
    "#   - PNG export of matrix for inclusion in Appendix and README\n",
    "#\n",
    "# Final predictions are collected into a list for saving in Section 7.4.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Extract features (X) and labels (y) from engineered metadata ------------\n",
    "X = df[[\"Latency\", \"Intensity\", \"AU_Count\"]]\n",
    "y = df[\"Emotion\"]\n",
    "\n",
    "# --- Re-import dependencies for reproducibility ------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define baseline classifiers ---------------------------------------------\n",
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# --- Initialize list to store predictions from all models --------------------\n",
    "all_preds = []\n",
    "\n",
    "# --- Train, Evaluate, and Visualize for Each Model ---------------------------\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Build modeling pipeline: Imputation ‚Üí Scaling ‚Üí Classifier\n",
    "    pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit model on training data and generate predictions on test data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "\n",
    "    # Print classification metrics to console\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Model: {name}\")\n",
    "    print(f\"üéØ Accuracy: {accuracy_score(y_test, preds):.3f}\")\n",
    "    print(f\"üßÆ F1 Score (Macro): {f1_score(y_test, preds, average='macro'):.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Generate confusion matrix and visualize results\n",
    "    cm = confusion_matrix(y_test, preds, labels=y.unique())\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y.unique())\n",
    "    disp.plot(xticks_rotation=45)\n",
    "    plt.title(f\"{name} ‚Äî Confusion Matrix\")\n",
    "\n",
    "    # Save confusion matrix as PNG to visuals folder\n",
    "    fig = disp.figure_\n",
    "    img_path = ROOT / \"outputs\" / \"visuals\" / f\"07_confmat_{name}.png\"\n",
    "    fig.savefig(img_path, bbox_inches=\"tight\")\n",
    "    print(f\"üì∏ Saved confusion matrix to: {img_path.name}\")\n",
    "\n",
    "    # Show the matrix inline\n",
    "    plt.show()\n",
    "\n",
    "    # Append predictions to all_preds list for future saving\n",
    "    all_preds.append(pd.DataFrame({\n",
    "        \"Model\": name,\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_pred\": preds\n",
    "    }))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "## 7.3.3 | Results Summary: Baseline Emotion Classifiers (LogReg, RF, KNN)\n",
    "\n",
    "By evaluating three traditional classifiers using engineered microexpression features ‚Äî **Latency**, **Intensity**, and **AU Count** ‚Äî to predict emotion class labels derived from CASME II and SMIC.\n",
    "\n",
    "## Confusion Matrix Observations\n",
    "- Strong prediction clusters emerged around broad categories such as **'others'**, **'negative'**, and **'sadness'**.\n",
    "- **Nuanced classes** (e.g., *repression*, *disgust*, *fear*) were often confused with one another, indicating poor separation in the feature space.\n",
    "- Some emotions (e.g., *happiness*) were **underrepresented or misclassified entirely**, suggesting the taxonomy fails to capture expressive distinctiveness.\n",
    "\n",
    "##  Performance Overview\n",
    "\n",
    "| Model   | Accuracy | F1 (Macro) |\n",
    "|---------|----------|------------|\n",
    "| LogReg  | ~0.375   | ~0.233     |\n",
    "| RF      | ~0.487   | ~0.282     |\n",
    "| KNN     | ~0.428   | ~0.244     |\n",
    "\n",
    "> üßæ Note: All metrics reflect stratified 80/20 splits across 560 combined microexpression records.\n",
    "\n",
    "##  Interpretation\n",
    "\n",
    "Despite preprocessing and model tuning, overall accuracy plateaued around **40‚Äì47%**, and macro F1 scores remained low. These outcomes reinforce our hypothesis that **surface-level metadata alone is insufficient** for distinguishing trauma-informed emotional states.\n",
    "\n",
    "The failure patterns observed in these confusion matrices support a deeper insight:  \n",
    "> **The current emotion taxonomy may be too vague to enable meaningful classification**, especially when expressions are masked, suppressed, or dissociative in nature.\n",
    "\n",
    "This aligns with the motivation behind our symbolic Z3 verification pipeline ‚Äî designed to address exactly these limitations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.4 Save Final Predictions for Z3 Symbolic Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell concatenates predictions from all 3 baseline models into a single\n",
    "# DataFrame (`all_preds`) and saves them to disk for use in Notebook 08.\n",
    "# These predictions will be cross-checked using symbolic empathy rules in Z3.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Define export path ------------------------------------------------------\n",
    "ROOT = Path.cwd().parent\n",
    "OUTPUT_PATH = ROOT / \"outputs\" / \"checks\" / \"z3_ready_input.parquet\"\n",
    "\n",
    "# --- Concatenate all predictions and export ----------------------------------\n",
    "z3_ready_df = pd.concat(all_preds, axis=0).reset_index(drop=True)\n",
    "z3_ready_df.to_parquet(OUTPUT_PATH, index=False)\n",
    "\n",
    "# --- üï∑Ô∏è Spider Check - Confirm save and display preview ----------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Z3-ready predictions saved to: {OUTPUT_PATH.name}\")\n",
    "print(f\"üìê Final shape: {z3_ready_df.shape}\")\n",
    "print(\"üìÑ Sample rows:\")\n",
    "display(z3_ready_df.sample(5))\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.5 Additional Classifier Benchmarks: SVC & MLP\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell extends the baseline evaluation by adding:\n",
    "#   - Support Vector Classifier (SVC)\n",
    "#   - Multi-Layer Perceptron (MLP)\n",
    "#\n",
    "# Both models follow the same pipeline: imputation + scaling + classification.\n",
    "# Results include accuracy, macro F1, and saved confusion matrices.\n",
    "# Final predictions are appended to `all_preds` for symbolic use in Notebook 08.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Re-import dependencies (if needed) --------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# --- Define additional models ------------------------------------------------\n",
    "additional_models = {\n",
    "    \"SVC\": SVC(probability=False, kernel=\"rbf\", C=1.0, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# --- Loop through additional models ------------------------------------------\n",
    "for name, model in additional_models.items():\n",
    "\n",
    "    # Build modeling pipeline with imputation + scaling\n",
    "    pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit and predict\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "\n",
    "    # Print performance\n",
    "    print(f\"üß† Model: {name}\")\n",
    "    print(\"üìä Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"üéØ F1 (macro):\", f1_score(y_test, preds, average=\"macro\"))\n",
    "\n",
    "    # Generate and save confusion matrix\n",
    "    cm = confusion_matrix(y_test, preds, labels=y.unique())\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y.unique())\n",
    "    disp.plot(xticks_rotation=45)\n",
    "    plt.title(f\"{name} ‚Äî Confusion Matrix\")\n",
    "\n",
    "    # Save figure\n",
    "    fig = disp.figure_\n",
    "    img_path = ROOT / \"outputs\" / \"visuals\" / f\"07_confmat_{name}.png\"\n",
    "    fig.savefig(img_path, bbox_inches=\"tight\")\n",
    "    print(f\"üì∏ Saved confusion matrix to: {img_path.name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Store predictions for export\n",
    "    all_preds.append(pd.DataFrame({\n",
    "        \"Model\": name,\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_pred\": preds\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.3.5 | Additional Model Results Summary: SVC & MLP\n",
    "\n",
    "Despite architectural differences, both SVC and MLP models produced **lower macro F1 scores** than previous classifiers. Their confusion matrices revealed:\n",
    "\n",
    "- Continued overlap in *others*, *sadness*, *repression*, and *fear*.\n",
    "- Frequent misclassification of *happiness* and *positive* emotions.\n",
    "- Low precision across nuanced emotional states.\n",
    "\n",
    "These results reinforce the core hypothesis of this framework: **surface-level metadata cannot resolve emotional ambiguity** without **semantic scaffolding.**\n",
    "\n",
    "---\n",
    "\n",
    "### Implication:\n",
    "Even nonlinear or boundary-aware models fail when emotional categories are vague or overlapping ‚Äî highlighting the need for symbolic logic (Z3) and trauma-aware verification rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3.6 Save Final Predictions for Z3 Symbolic Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "# Concatenates predictions from all 5 baseline classifiers:\n",
    "#   - Logistic Regression (LogReg)\n",
    "#   - Random Forest (RF)\n",
    "#   - K-Nearest Neighbors (KNN)\n",
    "#   - Support Vector Classifier (SVC)\n",
    "#   - Multi-Layer Perceptron (MLP)\n",
    "#\n",
    "# Output files:\n",
    "#   - `z3_ready_input.parquet`: for downstream symbolic reasoning in Notebook 08\n",
    "#   - `z3_ready_input.csv`: human-readable format for GitHub review and audit\n",
    "# Both formats preserve predicted labels and true labels for rule contradiction analysis.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Define export paths -----------------------------------------------------\n",
    "ROOT = Path.cwd().parent\n",
    "PARQUET_PATH = ROOT / \"outputs\" / \"checks\" / \"z3_ready_input.parquet\"\n",
    "CSV_PATH     = ROOT / \"outputs\" / \"checks\" / \"z3_ready_input.csv\"\n",
    "\n",
    "# --- Concatenate and save ----------------------------------------------------\n",
    "z3_ready_df = pd.concat(all_preds, axis=0).reset_index(drop=True)\n",
    "z3_ready_df.to_parquet(PARQUET_PATH, index=False)\n",
    "z3_ready_df.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# --- üï∑Ô∏è Spider Check (Sanity) -------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Z3-ready predictions saved:\")\n",
    "print(f\"   üìÅ {PARQUET_PATH.name}\")\n",
    "print(f\"   üìÑ {CSV_PATH.name}\")\n",
    "print(f\"‚úÖ Final shape: {z3_ready_df.shape}\")\n",
    "print(f\"‚úÖ Included models: {z3_ready_df['Model'].nunique()} ‚Äî {z3_ready_df['Model'].unique().tolist()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Display sample rows -----------------------------------------------------\n",
    "display(z3_ready_df.sample(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Executive Summary ‚Äî Symbolic Verification Milestone\n",
    "\n",
    "\n",
    "This notebook analyzed a fused dataset composed of 305 samples from **SMIC** and 255 samples from **CASME II**, two widely-used microexpression databases. All records were cleaned, aligned, and fused in Notebook 06 before being passed into this notebook for modeling. Feature engineering and emotion classification were conducted on this combined corpus using three engineered metadata features:\n",
    "\n",
    "- ‚úÖ **Latency** ‚Äî how fast the microexpression emerged  \n",
    "- ‚úÖ **Intensity** ‚Äî the emotional magnitude  \n",
    "- ‚úÖ **AU_Count** ‚Äî count of activated Action Units per clip  \n",
    "\n",
    "---\n",
    "\n",
    "### Baseline Classifier Overview\n",
    "\n",
    "Five classifiers were trained to predict emotion classes:\n",
    "- Logistic Regression (LogReg)\n",
    "- Random Forest (RF)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Classifier (SVC)\n",
    "- Multi-Layer Perceptron (MLP)\n",
    "\n",
    "All models used a standardized pipeline: `Impute ‚Üí Scale ‚Üí Classify`. The results were benchmarked using **Accuracy** and **F1 (Macro)** scores, and visualized via saved **Confusion Matrices**.\n",
    "\n",
    "---\n",
    "\n",
    "### Findings & Milestone Framing\n",
    "\n",
    "Across all five classifiers, performance plateaued between **~40‚Äì48% accuracy**, with **low macro F1**. Confusion matrices showed:\n",
    "\n",
    "- ‚ùå Persistent misclassification in nuanced emotions like *repression*, *disgust*, *sadness*, *fear*\n",
    "- ‚ùå Over-clustering in vague categories like *others* and *negative*\n",
    "- ‚ùå Frequent mislabeling of *positive* and *happiness* cases\n",
    "\n",
    "These findings **confirm the central hypothesis** of this trauma-informed AI framework:\n",
    "\n",
    "> **Surface-level metadata alone is insufficient** to detect trauma-influenced emotional states or subtle affective shifts.\n",
    "\n",
    "---\n",
    "\n",
    "### Implication & Handoff to Notebook 08\n",
    "\n",
    "These limitations motivated the shift to **symbolic logic verification** (Z3). All predictions have been exported to:\n",
    "\n",
    "üìÇ `z3_ready_input.parquet`\n",
    "\n",
    "This file now serves as the input to **23 symbolic empathy rules**, which will be used in Notebook 08 to cross-check ML predictions and uncover latent affective structures such as:\n",
    "\n",
    "- Suppression  \n",
    "- Dissociation  \n",
    "- Semantic Absence (*The Haunting Problem*)  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps ‚Äî Notebook 08\n",
    "\n",
    "- üîÑ Load and evaluate predictions from `z3_ready_input.parquet`\n",
    "- üß† Apply 23 empathy rules and log activation frequencies\n",
    "- üìä Cross-check symbolic flags vs. ML predictions\n",
    "- üö© Flag contradictions and analyze failure cases\n",
    "- üïµÔ∏è Identify ‚Äúfalse passes‚Äù where symbolic rules catch emotional masking missed by traditional models\n",
    "- ‚úçÔ∏è Update Glossary with emerging symbolic concepts (e.g., ‚Äúmasked sadness‚Äù, ‚Äúdefensive detachment‚Äù)\n",
    "\n",
    "> This marks the turning point:\n",
    "> From prediction to verification.  \n",
    "> From surface signals to symbolic safety.  \n",
    "> From unseen to **unforgotten**.\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix A | Baseline Classifier Confusion Matrices\n",
    "\n",
    "\n",
    "**Context:**  \n",
    "These matrices show how the baseline models (LogReg, RF, KNN, SVC, MLP) classified emotion labels using engineered metadata features.\n",
    "\n",
    "**Key Legend:**\n",
    "- Rows = Ground truth emotion labels  \n",
    "- Columns = Predicted emotion labels  \n",
    "- Diagonal = Correct predictions  \n",
    "- Off-diagonal = Misclassifications (ideally minimal)  \n",
    "\n",
    "**Notable Observations:**\n",
    "- ‚ùå High error in subtle classes (*e.g., disgust, repression, fear*)  \n",
    "- ‚ùå Over-clustering in generic categories (*e.g., others, negative*)  \n",
    "- ‚ö†Ô∏è Strong evidence the current emotion taxonomy is too coarse  \n",
    "- ‚úÖ Supports symbolic logic for finer affect classification  \n",
    "\n",
    "**Linked Visuals (Saved):**\n",
    "- `07_confmat_LogReg.png`  \n",
    "- `07_confmat_RF.png`  \n",
    "- `07_confmat_KNN.png`  \n",
    "- `07_confmat_SVC.png`  \n",
    "- `07_confmat_MLP.png`  \n",
    "\n",
    "---\n",
    "\n",
    "## Additional Model Results Summary: SVC & MLP\n",
    "\n",
    "\n",
    "To ensure a fair baseline before symbolic logic, two additional models were tested:\n",
    "\n",
    "| Model | Rationale |\n",
    "|-------|-----------|\n",
    "| **SVC** (Support Vector Classifier) | Evaluates class separability with kernel-based boundaries |\n",
    "| **MLP** (Multi-Layer Perceptron) | Tests nonlinear learning via neural architecture |\n",
    "\n",
    "**Result Highlights:**\n",
    "- üìâ Lower F1 scores than other models\n",
    "- üòµ Continued confusion among *others*, *sadness*, *repression*\n",
    "- üò¢ Misclassification of *happiness* and *positive* emotions\n",
    "- ‚úÖ Reinforces core hypothesis: metadata alone can‚Äôt resolve affective ambiguity\n",
    "\n",
    "**Implication:**  \n",
    "Even boundary-aware or nonlinear models fail when emotional categories are **semantically vague or overlapping** ‚Äî validating the need for **Z3 symbolic logic** and trauma-aware verification rules.\n",
    "\n",
    "---\n",
    "\n",
    "## Glossary of Metrics, Models, and Concepts\n",
    "\n",
    "\n",
    "| Term                     | Definition |\n",
    "|--------------------------|------------|\n",
    "| **Accuracy**             | Proportion of correct predictions |\n",
    "| **F1 Score (Macro)**     | Harmonic mean of precision/recall across all classes |\n",
    "| **Imputation (Median)**  | Fills missing values using the median of each feature |\n",
    "| **StandardScaler**       | Normalizes features to mean = 0, std = 1 |\n",
    "| **Logistic Regression**  | Linear model for classification |\n",
    "| **Random Forest**        | Ensemble of decision trees |\n",
    "| **K-Nearest Neighbors**  | Assigns class based on distance to nearest neighbors |\n",
    "| **SVC**                  | Classifies using maximum-margin hyperplanes (kernel-based) |\n",
    "| **MLP**                  | Feedforward neural net for nonlinear classification |\n",
    "| **AU_Count**             | Number of activated Action Units (facial muscle groups) |\n",
    "| **Latency**              | Time from onset to peak of a microexpression |\n",
    "| **Z3 Symbolic Logic**    | Formal logic system for verifying emotional states using trauma-informed rules |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trauma_ai)",
   "language": "python",
   "name": "trauma_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
