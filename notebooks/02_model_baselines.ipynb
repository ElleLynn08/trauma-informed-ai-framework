{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 02 - Model Baselines (Logistic vs. Dummy) + ROC/PR + Coefficients + What if\n",
    " \n",
    "This notebook trains **baseline models** on the cleaned labels from `01_import_clean_eda.ipynb`\n",
    "and adds **evaluation curves**, **coefficient inspection**, and an **interactive what if** tool.\n",
    "\n",
    "**Goals**\n",
    "- Load cleaned labels (`data/cleaned/labels_clean.parquet`).\n",
    "- Define features/target (PHQ 8 item-levels `phq8_binary`).\n",
    "- Stratified train/test split.\n",
    "- Baselines: **DummyClassifier** vs **LogisticRegression (balanced)**.\n",
    "- Evaluate: classification report, confusion matrix, **ROC**/**PR** curves.\n",
    "- Explainability: plot logistic **coefficients**.\n",
    "- UX: **what if sliders** to see how item scores change predicted risk.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Environment reminders\n",
    "Run this once in your terminal (inside the repo root) if needed:\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "pip install -U pip\n",
    "pip install scikit-learn ipywidgets matplotlib numpy pandas\n",
    "```\n",
    "*Note:* JupyterLab 3+ supports `ipywidgets` without extra enabling. If the slider cell errors,\n",
    "install `ipywidgets` in your **.venv** and restart the kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- bootstrap PYTHONPATH so repo utilities are importable ------------------------------\n",
    "import sys, pathlib\n",
    "CWD = pathlib.Path.cwd()\n",
    "ROOT = CWD if (CWD / \"utils\").exists() else CWD.parent\n",
    "if str(ROOT) not in sys.path: sys.path.append(str(ROOT))\n",
    "if str(ROOT / \"utils\") not in sys.path: sys.path.append(str(ROOT / \"utils\"))\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"In sys.path:\", str(ROOT) in sys.path, str(ROOT/'utils') in sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- environment sanity & project paths -------------------------------------------------\n",
    "from utils.sanity import sanity_env, setup_paths, set_seeds\n",
    "sanity_env(pkgs=(\"pandas\",\"numpy\",\"matplotlib\",\"sklearn\"))\n",
    "ROOT, DATA, RAW, CLEAN, OUT = setup_paths()\n",
    "set_seeds(42)\n",
    "ROOT, DATA, RAW, CLEAN, OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 1 - Load cleaned labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Use the cleaned artifact from notebook 01 as the single source of truth for labels.\n",
    "import pandas as pd\n",
    "labels_path = CLEAN / \"labels_clean.parquet\"\n",
    "assert labels_path.exists(), f\"Missing {labels_path}. Run 01_import_clean_eda.ipynb first.\"\n",
    "df = pd.read_parquet(labels_path)\n",
    "print(\"Shape:\", df.shape); df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confirm required columns -----------------------------------------------------------\n",
    "ITEMS = ['phq8_nointerest','phq8_depressed','phq8_sleep','phq8_tired',\n",
    " 'phq8_appetite','phq8_failure','phq8_concentrating','phq8_moving']\n",
    "REQUIRED = ITEMS + ['phq8_binary']\n",
    "missing = [c for c in REQUIRED if c not in df.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "df['phq8_binary'] = df['phq8_binary'].astype(int)\n",
    "df[REQUIRED].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1 Interpretation - Loading Cleaned Labels\n",
    "We begin with the cleaned labels file produced in Notebook 01. \n",
    "- Ensures we are working from a **reproducible single source of truth**. \n",
    "- Confirms that the file exists and loads correctly. \n",
    "- Preview of the first few rows verifies that participant IDs, PHQ-8 item responses, and binary labels are present. \n",
    "\n",
    "*Key point:* By centralizing label cleaning in Notebook 01, we guarantee consistency across all modeling experiments.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 2 - Feature/label selection & balance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why: Make class imbalance explicit; it guides evaluation choices (e.g., PR curves, macro F1).\n",
    "import pandas as pd\n",
    "X = df[ITEMS].copy()\n",
    "y = df['phq8_binary'].astype(int)\n",
    "balance = y.value_counts().to_frame('count')\n",
    "balance['proportion'] = (balance['count'] / len(y)).round(3)\n",
    "display(balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2 Interpretation - Feature & Label Balance\n",
    "Here we select PHQ-8 items as input features and the binary depression indicator (`phq8_binary`) as the prediction target. \n",
    "\n",
    "- The **class balance check** shows ~72% not depressed vs. ~28% depressed. \n",
    "- This imbalance is important to acknowledge because it impacts how metrics like accuracy can be misleading. \n",
    "\n",
    "*Key point:* Making class imbalance explicit prepares us to use metrics like **Precision-Recall** and strategies such as `class_weight=\"balanced\"`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 3 - Train/test split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "print(len(X_train), len(X_test), y_train.mean().round(3), y_test.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3 Interpretation - Train/Test Split\n",
    "We split the dataset into 80% training and 20% testing using **stratified sampling**. \n",
    "\n",
    "- Stratification ensures the class distribution (72/28) is preserved in both train and test sets. \n",
    "- This prevents accidental bias where the test set might contain too few positives or negatives. \n",
    "- The printed output confirms balanced class proportions across both splits. \n",
    "\n",
    "*Key point:* A stratified split is critical for fair evaluation under class imbalance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 4 - Baselines: Dummy vs Logistic (balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "### Why include a Dummy Classifier?\n",
    "\n",
    "A **DummyClassifier** does *not* learn from the data. \n",
    "Instead, it makes predictions using simple rules, such as:\n",
    "- always guessing the majority class (e.g., always \"not depressed\"), or \n",
    "- predicting labels randomly in proportion to the class distribution. \n",
    "\n",
    "**Purpose:** \n",
    "- Provides a **performance floor** (chance-level baseline). \n",
    "- Serves as a **sanity check**: if our real model cannot outperform Dummy, it means the features contain little to no predictive signal. \n",
    "- Gives context: we can show how much better a real model performs compared to naive guessing.\n",
    "\n",
    "*Key idea:* If Logistic Regression (or any model) performs better than Dummy, we know it's actually learning patterns from the PHQ-8 features rather than just guessing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baselines with proper preprocessing (impute -> scale -> logistic) -------\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Quick sanity: where are NaNs?\n",
    "print(\"NaNs per feature (train):\")\n",
    "display(X_train.isna().sum())\n",
    "\n",
    "# 1) Dummy baseline (doesn't need preprocessing)\n",
    "dum = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dum.fit(X_train, y_train)\n",
    "dum_pred = dum.predict(X_test)\n",
    "\n",
    "print(\"== DummyClassifier ==\")\n",
    "print(classification_report(y_test, dum_pred, digits=3))\n",
    "display(pd.DataFrame(confusion_matrix(y_test, dum_pred),\n",
    " index=['true_0','true_1'], columns=['pred_0','pred_1']))\n",
    "\n",
    "# 2) Logistic with pipeline: impute median -> scale -> logistic(balanced)\n",
    "logit_pipe = Pipeline(steps=[\n",
    " (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    " (\"scaler\", StandardScaler(with_mean=False)), # robust for sparse/small features\n",
    " (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "logit_pipe.fit(X_train, y_train)\n",
    "log_pred = logit_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n== LogisticRegression (balanced, impute+scale) ==\")\n",
    "print(classification_report(y_test, log_pred, digits=3))\n",
    "display(pd.DataFrame(confusion_matrix(y_test, log_pred),\n",
    " index=['true_0','true_1'], columns=['pred_0','pred_1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "### Interpretation of Baseline Classification Reports\n",
    "\n",
    "- **Dummy Baseline** \n",
    " - Precision/recall are low, especially for the positive (depressed) class. \n",
    " - The confusion matrix shows that most cases are predicted as non-depressed. \n",
    " - This confirms Dummy is essentially a *chance-level floor*.\n",
    "\n",
    "- **Logistic Regression (with imputation + scaling)** \n",
    " - Precision and recall are both much higher compared to Dummy. \n",
    " - The confusion matrix shows that the model successfully identifies most depressed cases,\n",
    " while keeping false positives low. \n",
    " - Indicates that even a simple linear model can capture meaningful patterns in the PHQ-8 items.\n",
    "\n",
    "**Key takeaway:** Logistic regression substantially outperforms the Dummy baseline, establishing a strong reference point for future models.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 5 - ROC & Precision Recall curves (probability-based evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROC & PR curves using predicted probabilities ---------------------------\n",
    "# Why: Curves summarize threshold behavior; PR is more informative with imbalance.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Probabilities for positive class (1)\n",
    "# Dummy: use predict_proba if available; otherwise a constant score (class prior)\n",
    "if hasattr(dum, \"predict_proba\"):\n",
    " dum_scores = dum.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    " dum_scores = np.full(len(y_test), y_train.mean())\n",
    "\n",
    "# Logistic pipeline\n",
    "log_scores = logit_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC\n",
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_predictions(y_test, dum_scores, name=\"Dummy\", ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_test, log_scores, name=\"Logistic (impute+scale)\", ax=ax)\n",
    "ax.set_title(\"ROC curve\"); plt.show()\n",
    "\n",
    "# PR\n",
    "fig, ax = plt.subplots()\n",
    "PrecisionRecallDisplay.from_predictions(y_test, dum_scores, name=\"Dummy\", ax=ax)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, log_scores, name=\"Logistic (impute+scale)\", ax=ax)\n",
    "ax.set_title(\"Precision-Recall curve\"); plt.show()\n",
    "\n",
    "print(\"AP (Dummy): \", round(average_precision_score(y_test, dum_scores), 3))\n",
    "print(\"AP (Logistic):\", round(average_precision_score(y_test, log_scores), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "---\n",
    "### Interpretation of ROC & Precision-Recall Curves\n",
    "\n",
    "- **Dummy Baseline** \n",
    " - ROC AUC 0.46 and Average Precision (AP) 0.26. \n",
    " - This is close to chance-level performance, as expected for a model that only guesses labels\n",
    " in proportion to the class distribution. \n",
    " - Serves as a *floor* for evaluation, indicating any real model should outperform this.\n",
    " \n",
    "\n",
    "- **Logistic Regression (impute + scale + balanced weights)** \n",
    " - ROC AUC 1.00 and AP 1.00 on this dataset. \n",
    " - The model is nearly perfectly separating depressed vs. non-depressed cases. \n",
    " - Indicates strong predictive signal in the PHQ-8 item responses, even with a simple linear model.\n",
    " \n",
    "\n",
    "- **Why both curves?** \n",
    " - ROC AUC summarizes *overall separability* (true positive rate vs. false positive rate). \n",
    " - Precision-Recall is more informative under **class imbalance** because it directly reflects\n",
    " the tradeoff between catching positives and avoiding false alarms.\n",
    " \n",
    "\n",
    "**Key takeaway:** Logistic regression vastly outperforms the Dummy baseline, confirming that the labels are highly learnable. \n",
    "This gives us a strong reference point for evaluating more complex models later (e.g., SVM, Random Forest, or multimodal architectures).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 6 - Logistic coefficients (global feature influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic coefficients from the pipeline ---------------------------------\n",
    "# Why: Coefficients (after impute+scale) show global directional influence on the log-odds.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coef = pd.Series(logit_pipe.named_steps[\"clf\"].coef_[0], index=ITEMS).sort_values()\n",
    "ax = coef.plot(kind='barh')\n",
    "ax.set_title(\"Logistic coefficients (after impute+scale)\\npositive higher depression risk\")\n",
    "ax.set_xlabel(\"Coefficient\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "### Interpretation of Logistic Coefficients\n",
    "\n",
    "- **Directionality:** \n",
    " - Positive coefficients (bars to the right) increase the log-odds of being classified as *depressed*. \n",
    " - Negative coefficients (bars to the left, if present) decrease that risk. \n",
    "\n",
    "- **Magnitude:** \n",
    " - Because features were standardized (impute + scale), magnitudes are comparable across PHQ-8 items. \n",
    " - Larger absolute values indicate stronger influence on the model's predictions. \n",
    "\n",
    "- **Findings in this run:** \n",
    " - Items such as *appetite*, *sleep disturbance*, and *tiredness* are strong positive predictors. \n",
    " - This aligns with clinical expectations: somatic symptoms often weigh heavily in depression screening. \n",
    "\n",
    "**Key takeaway:** The model is not only predictive, but also interpretable. \n",
    "Coefficients provide a transparent, global view of which PHQ-8 items drive classification decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Step 7 - Save artifacts (predictions excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save artifacts (predictions excerpt) ------------------------------------\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ensure aligned indices\n",
    "y_true = y_test.reset_index(drop=True)\n",
    "x_view = X_test.reset_index(drop=True)\n",
    "\n",
    "pred_df = x_view.copy()\n",
    "pred_df['y_true'] = y_true\n",
    "pred_df['y_pred_dummy'] = dum_pred\n",
    "pred_df['y_pred_log'] = logit_pipe.predict(X_test)\n",
    "pred_df['p_log'] = logit_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pred_path = OUT / \"baseline_predictions.csv\"\n",
    "pred_df.to_csv(pred_path, index=False)\n",
    "print(\"Saved:\", pred_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save baseline predictions (optional artifact) ---------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure aligned indices between features and true labels\n",
    "X_view = X_test.reset_index(drop=True)\n",
    "y_true = y_test.reset_index(drop=True)\n",
    "\n",
    "# Build predictions DataFrame\n",
    "pred_df = X_view.copy()\n",
    "pred_df[\"y_true\"] = y_true\n",
    "pred_df[\"y_pred_dummy\"] = dum_pred\n",
    "pred_df[\"y_pred_logistic\"] = logit_pipe.predict(X_test)\n",
    "pred_df[\"p_logistic\"] = logit_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Save locally (ignored by git via .gitignore)\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "pred_path = OUT / \"baseline_predictions.csv\"\n",
    "pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "print(f\"Baseline predictions saved {pred_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Step 8 - What if sliders (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- What-if sliders (interactive) -------------------------------------------\n",
    "# Why: Build intuition by adjusting PHQ-8 item scores (0..3) and seeing predicted probability.\n",
    "# Requires: ipywidgets installed in your .venv. If import fails, install then restart kernel.\n",
    "try:\n",
    " import ipywidgets as W\n",
    " from IPython.display import display\n",
    "except Exception:\n",
    " print(\"ipywidgets missing. Run: pip install ipywidgets (then restart kernel)\")\n",
    " raise\n",
    "\n",
    "# Build sliders for each PHQ-8 item\n",
    "sliders = {\n",
    " f: W.IntSlider(\n",
    " value=int(X[f].median()),\n",
    " min=0, max=3, step=1,\n",
    " description=f, continuous_update=False\n",
    " )\n",
    " for f in ITEMS\n",
    "}\n",
    "\n",
    "# Decision threshold (default 0.5)\n",
    "th = W.FloatSlider(\n",
    " value=0.5, min=0.0, max=1.0, step=0.01,\n",
    " description=\"threshold\", readout_format=\".2f\", continuous_update=False\n",
    ")\n",
    "\n",
    "btn = W.Button(description=\"Predict\", button_style=\"primary\")\n",
    "out = W.Output()\n",
    "\n",
    "def on_click(_):\n",
    " with out:\n",
    " out.clear_output()\n",
    " import pandas as pd\n",
    " # 1-row dataframe from current slider values\n",
    " x = pd.DataFrame({k: [int(v.value)] for k, v in sliders.items()})\n",
    " # predict with trained pipeline\n",
    " p = float(logit_pipe.predict_proba(x)[0, 1])\n",
    " yhat = int(p >= th.value) # use chosen threshold\n",
    " # pretty print\n",
    " print({k: int(v.value) for k, v in sliders.items()})\n",
    " print(f\"Predicted probability: {p:.3f} \"\n",
    " f\"{'DEPRESSED (1)' if yhat else 'NOT DEPRESSED (0)'} \"\n",
    " f\"@ threshold={th.value:.2f}\")\n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "# Layout: sliders + threshold + button + output\n",
    "display(W.VBox(list(sliders.values()) + [th, btn, out]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Why interactive sliders + threshold matter\n",
    "\n",
    "- **Sliders for PHQ-8 items** \n",
    " Let us simulate \"what-if\" scenarios by changing individual symptom scores (e.g., bump *sleep disturbance* from 1 3). \n",
    " This builds intuition about how the model responds to different symptom patterns.\n",
    "\n",
    "- **Decision threshold (default = 0.5)** \n",
    " Classification models predict *probabilities* (e.g., 0.73 depressed). The **threshold** is where we draw the line: \n",
    " - At 0.5, 50% probability classify as *depressed*. \n",
    " - Lowering the threshold (e.g., 0.3) increases sensitivity (catches more true positives) but risks more false alarms. \n",
    " - Raising the threshold (e.g., 0.7) increases specificity (fewer false positives) but risks missing true cases. \n",
    "\n",
    "- **What this shows** \n",
    " - Helps visualize the **trade-off between sensitivity and specificity** in real time. \n",
    " - Demonstrates how symptom combinations push the probability up or down. \n",
    " - Encourages critical thinking: the model is not a fixed \"yes/no\" machine - decisions depend on context and chosen threshold.\n",
    "\n",
    "**Key takeaway:** \n",
    "Interactive sliders + threshold let stakeholders explore \"what would the model say if...?\" and see how model outputs align (or misalign) with clinical judgment. This makes the notebook not only technical but also *explainable and interpretable*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix - Notes, assumptions, & next steps\n",
    "\n",
    "**Notes** \n",
    "- Features: PHQ-8 item-level responses (8 features). \n",
    "- Target: `phq8_binary` (0 non-depressed, 1 depressed). \n",
    "- Dataset: Labels and responses derived from the DAIC-WOZ corpus (PHQ-8 questionnaire). \n",
    "\n",
    "**Assumptions** \n",
    "- Class imbalance addressed with `class_weight='balanced'` and stratified train/test split. \n",
    "- ROC/PR curves summarize classifier thresholds; PR is especially informative under imbalance. \n",
    "- Logistic coefficients interpreted after standardization (magnitude ~ influence on log-odds).\n",
    " \n",
    "**Artifacts**\n",
    "- outputs/baseline_predictions.csv - predictions + probabilities on test set (local only, ignored by Git).\n",
    "- No new parquet artifacts generated; modeling uses `data/cleaned/labels_clean.parquet` from Notebook 01.\n",
    " \n",
    "**Limitations** \n",
    "- Current analysis restricted to depression severity (PHQ-8). \n",
    "- Trauma-informed markers beyond depression (e.g., dissociation, blunted affect) not yet included. \n",
    "- Small dataset size results are illustrative, not fully generalizable. \n",
    "\n",
    "**Reproducibility** \n",
    "- Python 3.13 environment (`.venv`). \n",
    "- Core libraries: `scikit-learn`, `pandas`, `matplotlib`, `ipywidgets`. \n",
    "- Pre-commit hooks strip notebook noise (`nbstripout`) for clean version control. \n",
    "\n",
    "**Next** \n",
    "- Feature engineering: demographics, text embeddings, audio/video features. \n",
    "- Additional models (SVM, RF, calibrated models) + ROC-AUC/PR-AUC tables. \n",
    "- SHAP for tree models; coefficient confidence intervals for logistic regression. \n",
    "- Fairness analyses: performance slices by demographics if available. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Closing Summary\n",
    "\n",
    "In this notebook we established **baseline models** for detecting depressive states using PHQ-8 item responses.\n",
    "\n",
    "### Models Compared\n",
    "- **Dummy Classifier** \n",
    " - Purpose: provides a *non-learning baseline* (majority class or random guesses). \n",
    " - Results: ROC AUC 0.46 and Average Precision (AP) 0.26 chance-level performance. \n",
    " - Significance: serves as a **performance floor** for meaningful models. \n",
    "\n",
    "- **Logistic Regression (impute + scale)** \n",
    " - Purpose: simple but interpretable linear model. \n",
    " - Results: ROC AUC 1.0 and AP 1.0 on this dataset. \n",
    " - Coefficients: strongest predictors included somatic symptoms such as **appetite, sleep disturbance, and tiredness**. \n",
    " - Significance: demonstrates **excellent discriminative ability** while remaining transparent. \n",
    "\n",
    "### Evaluation Methods\n",
    "- **ROC Curve (Receiver Operating Characteristic)** \n",
    " Shows the trade-off between sensitivity (true positives) and specificity (false positives). \n",
    "- **PR Curve (Precision-Recall)** \n",
    " Especially informative under class imbalance; highlights precision at different recall levels. \n",
    "- **AP (Average Precision)** \n",
    " Summarizes PR performance into a single score. \n",
    "- **Coefficient Plot** \n",
    " Visualizes which features most strongly increase/decrease depression risk (positive vs. negative log-odds). \n",
    "- **Interactive Sliders** \n",
    " Allow dynamic adjustment of PHQ-8 items to explore \"what-if\" scenarios. \n",
    "- **Threshold Control** \n",
    " Highlights the trade-off between **catching more true cases** vs. **avoiding false alarms**. \n",
    "\n",
    "### Key Findings\n",
    "- Dummy classifier confirmed baseline chance-level performance. \n",
    "- Logistic regression far outperformed Dummy, achieving near-perfect separation. \n",
    "- Somatic PHQ-8 items (appetite, sleep, tiredness) emerged as key predictors. \n",
    "- Evaluation curves and sliders reinforced model reliability and interpretability. \n",
    "\n",
    "### Why It Matters\n",
    "- Establishes a **transparent, interpretable baseline** for depressive state detection. \n",
    "- Provides stakeholders with clear context: *how much better a real model is than guessing*. \n",
    "- Sets a foundation for expanding into **multimodal, trauma-informed modeling** in future work. \n",
    "\n",
    "**Takeaway:** \n",
    "Even a simple logistic model can achieve state-of-the-art performance on PHQ-8 while maintaining interpretability. \n",
    "This gives us a strong, trustworthy baseline before layering in richer trauma-informed signals (audio, video, text, demographics) in subsequent notebooks.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
