{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook 03 - Feature Engineering & Multimodal Inputs\n",
    "\n",
    "**Project:** Recognizing the Unseen - A Multimodal, Trauma-Informed AI Framework \n",
    "**Goal of this notebook:** engineer features beyond PHQ-8 and prepare multimodal inputs (text, audio, video) for downstream modeling.\n",
    "\n",
    "**Builds on:** \n",
    "- Notebook 01: Import, clean, EDA (labels + minimal cleaning) \n",
    "- Notebook 02: Baselines (Dummy vs. Logistic), ROC/PR, coefficient plots, interactive sliders + thresholds \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "## Contents\n",
    "1. Data sources & setup \n",
    "2. SMT guardrails (Z3) for data integrity and split hygiene \n",
    "3. Feature engineering \n",
    " - Tabular (PHQ-8) \n",
    " - Text (transcripts embeddings) \n",
    " - Audio (prosody) \n",
    " - Video (facial action units) \n",
    "4. Multimodal dataset assembly \n",
    "5. Artifacts (saved processed data) \n",
    "6. Limitations & reproducibility \n",
    "7. Closing summary & next steps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data sources & setup\n",
    "\n",
    "Load the cleaned PHQ-8 labels and set up placeholders for additional modalities. \n",
    "This cell focuses on reading already-prepared artifacts from prior notebooks and defining\n",
    "conventions for participant/session keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports & Paths  +  Data sources & setup (canonical one-stop block)\n",
    "# -----------------------------------------------------------------------------\n",
    "# - Imports (numpy/pandas, etc.) + quick diagnostics (Python/pandas version)\n",
    "# - Resolve repo root and standard data directories\n",
    "# - Define canonical join/target names used across Notebook 03\n",
    "# - Load cleaned labels (from Notebook 01/02 output) with graceful fallback\n",
    "# - Normalize common column-name variants -> {participant_id, label, split}\n",
    "# - Print a concise summary for reviewers\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports -----------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# (Optional) quick diagnostics to make runtime context explicit\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "# Optional: show whether any BLAS info is available (won't crash if missing)\n",
    "blas = getattr(getattr(np, \"__config__\", object()), \"blas_opt_info\", {})\n",
    "print(\"BLAS info found:\", bool(blas))\n",
    "\n",
    "# --- Paths (single source of truth) ------------------------------------------\n",
    "# Reuse ROOT_DIR if already set (e.g., by earlier cells); otherwise derive it.\n",
    "try:\n",
    "    ROOT_DIR\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().resolve().parent  # notebooks/ -> repo root\n",
    "\n",
    "DATA_DIR      = ROOT_DIR / \"data\"\n",
    "CLEAN_DIR     = DATA_DIR / \"cleaned\"       # outputs from NB01/NB02 (adjust if your layout differs)\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"     # downstream features go here\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Canonical names & expected labels artifact ------------------------------\n",
    "JOIN_KEY = \"participant_id\"                # canonical join key used throughout NB03\n",
    "TARGET   = \"label\"                         # canonical binary target (0/1)\n",
    "LABELS_PATH = CLEAN_DIR / \"labels_clean.parquet\"   # <- adjust here if NB01/02 writes elsewhere\n",
    "\n",
    "# --- Load labels with guardrails (degrade gracefully if file not ready) ------\n",
    "if LABELS_PATH.exists():\n",
    "    labels_df = pd.read_parquet(LABELS_PATH)\n",
    "    print(f\"Loaded labels: {LABELS_PATH} | shape={labels_df.shape}\")\n",
    "else:\n",
    "    print(f'NOTE: labels_clean.parquet not found -> {LABELS_PATH}')\n",
    "    print(\"      (Run Notebook 01/02 to generate, or adjust LABELS_PATH if it moved.)\")\n",
    "    # Create empty skeleton so downstream checks in Step 2 can SKIP cleanly\n",
    "    labels_df = pd.DataFrame(columns=[JOIN_KEY, TARGET, \"split\"])\n",
    "\n",
    "# --- Normalize column names to our canonical schema --------------------------\n",
    "rename_map = {}\n",
    "# join key variants\n",
    "if \"subject_id\" in labels_df.columns and JOIN_KEY not in labels_df.columns:\n",
    "    rename_map[\"subject_id\"] = JOIN_KEY\n",
    "if \"id\" in labels_df.columns and JOIN_KEY not in labels_df.columns:\n",
    "    rename_map[\"id\"] = JOIN_KEY\n",
    "# target variants\n",
    "if \"target\" in labels_df.columns and TARGET not in labels_df.columns:\n",
    "    rename_map[\"target\"] = TARGET\n",
    "if \"phq8_binary\" in labels_df.columns and TARGET not in labels_df.columns:\n",
    "    rename_map[\"phq8_binary\"] = TARGET\n",
    "\n",
    "if rename_map:\n",
    "    labels_df = labels_df.rename(columns=rename_map)\n",
    "\n",
    "# Ensure placeholders exist (keeps downstream guardrails readable & safe)\n",
    "if JOIN_KEY not in labels_df.columns:\n",
    "    labels_df[JOIN_KEY] = pd.Series(dtype=\"object\")\n",
    "    print(f'NOTE: added empty \"{JOIN_KEY}\" column')\n",
    "if TARGET not in labels_df.columns:\n",
    "    labels_df[TARGET] = pd.Series(dtype=\"Int64\")  # nullable int; matches {0,1}\n",
    "    print(f'NOTE: added empty \"{TARGET}\" column')\n",
    "if \"split\" not in labels_df.columns:\n",
    "    labels_df[\"split\"] = pd.Series(dtype=\"string\")\n",
    "    print('NOTE: added empty \"split\" column')\n",
    "\n",
    "# --- Reviewer-friendly summary ----------------------------------------------\n",
    "n_rows = len(labels_df)\n",
    "sample_cols = [c for c in [JOIN_KEY, TARGET, \"split\"] if c in labels_df.columns]\n",
    "print(f\"labels_df: {n_rows} rows, {labels_df.shape[1]} cols | has {sample_cols} | head:\")\n",
    "print(labels_df[sample_cols].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) SMT guardrails (Z3) for data integrity and split hygiene\n",
    "\n",
    "We add lightweight **formal checks** to catch structural mistakes early:\n",
    "\n",
    "- Temporal event sanity: `onset < apex < offset n_frames - 1` \n",
    "- Window safety: each feature window stays within clip bounds \n",
    "- Sampling consistency: `fps > 0` and `duration frames / fps` \n",
    "- Split hygiene: subject-disjoint train/val/test; minimum class presence per split \n",
    "- Label domain checks: labels belong to the expected set\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2 - SMT GUARDRAILS (Z3) + SPLIT HYGIENE\n",
    "# Goal:\n",
    "#   - Enforce label domain and (optionally) split integrity with small, readable\n",
    "#     checks that fail-fast when assumptions break.\n",
    "#   - Keep notebook executable even when artifacts are not ready (print & skip).\n",
    "# Why:\n",
    "#   - Early structural checks catch silent drift (e.g., wrong label domain, ID\n",
    "#     overlap across splits) before modeling.\n",
    "# =============================================================================\n",
    "\n",
    "# Make repo-root imports work from inside notebooks/\n",
    "# Why: the kernel's CWD is often `notebooks/`, while `verification.py` lives at repo root.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_repo_root(filename: str = \"verification.py\") -> Path | None:\n",
    "    \"\"\"Walk upward from CWD until `filename` is found; return its parent (repo root).\"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / filename).exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Reuse ROOT_DIR from Step 1 if present; otherwise resolve it robustly here.\n",
    "try:\n",
    "    ROOT_DIR\n",
    "except NameError:\n",
    "    ROOT_DIR = _find_repo_root() or Path.cwd().resolve().parent  # fallback: notebooks/ -> repo root\n",
    "\n",
    "# Ensure the root is importable\n",
    "if ROOT_DIR and str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Now safe(ish) to import guardrail utilities; if unavailable, degrade gracefully.\n",
    "_guardrails_loaded = False\n",
    "try:\n",
    "    from verification import (\n",
    "        check_event_triplet,             # example timing check for onset/apex/offset\n",
    "        check_window_bounds,             # window [start, start+len) within [0, n)\n",
    "        check_sampling_consistency,      # duration  frames/fps\n",
    "        assert_disjoint_splits,          # no subject overlap across splits\n",
    "        min_class_presence,              # per-split class counts  threshold\n",
    "        assert_label_domain,             # labels in allowed set\n",
    "        verify_env,                      # tiny runtime report for smoke tests/CI\n",
    "    )\n",
    "    _guardrails_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"SKIP: guardrail utilities not importable ({type(e).__name__}: {e}). \"\n",
    "          \"Proceeding without hard checks so the notebook stays runnable.\")\n",
    "\n",
    "JOIN_KEY = \"participant_id\"\n",
    "TARGET   = \"label\"\n",
    "\n",
    "# ---- 2.1 Label hygiene ------------------------------------------------------\n",
    "if not _guardrails_loaded:\n",
    "    print(\"SKIP: label checks (verification.py not loaded).\")\n",
    "elif \"labels_df\" not in globals() or labels_df is None or labels_df.empty or TARGET not in labels_df.columns:\n",
    "    print(\"SKIP: label checks (labels_df empty or target column missing).\")\n",
    "else:\n",
    "    # Domain guarantee  reviewers see intent: binary classification (0/1).\n",
    "    assert_label_domain(labels_df[TARGET], allowed=(0, 1))\n",
    "    print(\"OK: label domain is restricted to {0, 1}.\")\n",
    "\n",
    "# ---- 2.2 Split hygiene (optional) -------------------------------------------\n",
    "# If you already created a split in Notebook 02, this validates it.\n",
    "if not _guardrails_loaded:\n",
    "    print(\"SKIP: split checks (verification.py not loaded). \"\n",
    "          \"Create deterministic splits in Notebook 02/03 before modeling.\")\n",
    "elif (\"labels_df\" in globals() and labels_df is not None and not labels_df.empty\n",
    "      and (\"split\" in labels_df.columns) and (JOIN_KEY in labels_df.columns)):\n",
    "    # Extract subject IDs per split (keeps checks explainable & auditable).\n",
    "    train_ids = labels_df.loc[labels_df[\"split\"] == \"train\", JOIN_KEY]\n",
    "    val_ids   = labels_df.loc[labels_df[\"split\"] == \"val\",   JOIN_KEY]\n",
    "    test_ids  = labels_df.loc[labels_df[\"split\"] == \"test\",  JOIN_KEY]\n",
    "\n",
    "    # (a) No subject overlap across splits\n",
    "    assert_disjoint_splits(train_ids, val_ids, test_ids)\n",
    "    print(\"OK: no subject overlap across splits (train/val/test).\")\n",
    "\n",
    "    # (b) Minimum per-class support in each split  guards against degenerate folds\n",
    "    min_class_presence(\n",
    "        {\n",
    "            \"train\": labels_df.loc[labels_df[\"split\"] == \"train\", TARGET],\n",
    "            \"val\":   labels_df.loc[labels_df[\"split\"] == \"val\",   TARGET],\n",
    "            \"test\":  labels_df.loc[labels_df[\"split\"] == \"test\",  TARGET],\n",
    "        },\n",
    "        min_count=5  # Adjust with dataset size; aim to preserve evaluation stability.\n",
    "    )\n",
    "    print(\"OK: each split meets minimum class presence thresholds.\")\n",
    "else:\n",
    "    print('SKIP: split checks (no \"split\" column yet). '\n",
    "          \"Create deterministic splits in Notebook 02/03 before modeling.\")\n",
    "\n",
    "# ---- 2.3 Timing/window sanity (optional, runs only if variables provided) ---\n",
    "# These are examples; they will quietly skip if you haven't defined the inputs yet.\n",
    "# Rationale: keeps nbconvert/CI green while still documenting expectations.\n",
    "\n",
    "if _guardrails_loaded:\n",
    "    # Example A: sampling consistency for a video segment: frames / fps  duration\n",
    "    try:\n",
    "        ok, msg = check_sampling_consistency(\n",
    "            frames=int(video_frames),        # define upstream when available\n",
    "            fps=float(video_fps),\n",
    "            duration_sec=float(video_duration_sec)\n",
    "        )\n",
    "        print(\"Video sampling check:\", msg)\n",
    "    except Exception:\n",
    "        # Not available yet; that is expected in early drafts.\n",
    "        pass\n",
    "\n",
    "    # Example B: generic window bounds (e.g., feature extraction slices)\n",
    "    try:\n",
    "        ok, msg = check_window_bounds(\n",
    "            start=int(win_start),            # define upstream when available\n",
    "            length=int(win_len),\n",
    "            n_frames=int(total_frames)\n",
    "        )\n",
    "        print(\"Window bounds check:\", msg)\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"SKIP: timing/window checks (verification.py not loaded).\")\n",
    "\n",
    "print(\"Guardrail checks completed.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Smoke test - confirm guardrail utilities are importable and show env facts\n",
    "# =============================================================================\n",
    "_loaded = globals().get(\"_guardrails_loaded\", False)\n",
    "\n",
    "if _loaded:\n",
    "    try:\n",
    "        import verification\n",
    "        print(f\"Verification module loaded from: {verification.__file__}\")\n",
    "        want = [\n",
    "            \"check_event_triplet\",\n",
    "            \"check_window_bounds\",\n",
    "            \"check_sampling_consistency\",\n",
    "            \"assert_disjoint_splits\",\n",
    "            \"min_class_presence\",\n",
    "            \"assert_label_domain\",\n",
    "            \"verify_env\",\n",
    "        ]\n",
    "        available = [name for name in want if getattr(verification, name, None)]\n",
    "        missing   = [name for name in want if name not in available]\n",
    "        print(\"Available guardrail functions:\", available)\n",
    "        if missing:\n",
    "            print(\"Note: missing in verification.py ->\", missing)\n",
    "        # One-line environment report (nice for CI and Dr. S)\n",
    "        try:\n",
    "            print(\"Env:\", verification.verify_env())\n",
    "        except Exception:\n",
    "            print(\"Env: verify_env() raised; skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Smoke test warning: import succeeded but inspection failed ({type(e).__name__}: {e})\")\n",
    "else:\n",
    "    print(\"Smoke test: verification.py not loaded (see SKIP messages above).\")\n",
    "\n",
    "# Show where ROOT_DIR resolved to (useful for CI/review logs)\n",
    "print(\"Resolved ROOT_DIR:\", ROOT_DIR if \"ROOT_DIR\" in globals() else \"<not set>\")\n",
    "\n",
    "# Peek at the first few sys.path entries to confirm import order\n",
    "print(\"sys.path[0:3]:\", sys.path[:3])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "> ðŸ’¡ **Workflow tip:** Run the checks immediately after loading each modality. Fail fast with clear errors so issues don't propagate into modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Feature engineering\n",
    "We create modality-specific features. Start simple and keep everything **reproducible**.\n",
    "\n",
    "### 3.1 Tabular (PHQ-8)\n",
    "- Standardize numeric PHQ-8 items.\n",
    "- (Optional) Create low-order interaction terms for hypothesis-driven pairs.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick probe: see all columns that look PHQ-related\n",
    "[c for c in labels_df.columns if \"phq\" in str(c).lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 Tabular (PHQ-8) - Clinical-style imputation + optional rounding\n",
    "# Goal:\n",
    "#   - Build interpretable PHQ-8 features (sum/mean/missingness, z-scores).\n",
    "#   - Clinical scoring:\n",
    "#       * If 1 item missing  impute that item with the row mean, then sum.\n",
    "#       * If 2 items missing  leave score NaN (no aggressive imputation).\n",
    "#   - Optional rounding of the final score to match reporting conventions.\n",
    "#   - After scoring, zero-fill item columns for downstream models (documented).\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "TAB_OUT = PROCESSED_DIR / \"tabular_phq8.parquet\"\n",
    "\n",
    "# ---- Explicit PHQ-8 schema pin (order matters: items 1..8) ------------------\n",
    "PHQ8_COLS = [\n",
    "    \"phq8_nointerest\",      # 1  little interest/pleasure\n",
    "    \"phq8_depressed\",       # 2  feeling down/depressed/hopeless\n",
    "    \"phq8_sleep\",           # 3  sleep problems\n",
    "    \"phq8_tired\",           # 4  low energy/tired\n",
    "    \"phq8_appetite\",        # 5  appetite/eating\n",
    "    \"phq8_failure\",         # 6  feeling bad/failure/worthless/guilty\n",
    "    \"phq8_concentrating\",   # 7  trouble concentrating\n",
    "    \"phq8_moving\",          # 8  psychomotor (restless/slow)\n",
    "]\n",
    "REQUIRE_ALL_ITEMS = True  # set False to proceed if any items missing\n",
    "\n",
    "# Choose rounding for the total score: \"nearest\" | \"bankers\" | \"floor\" | \"ceil\" | None\n",
    "SCORE_ROUNDING = \"nearest\"\n",
    "\n",
    "# ---- Guard schema presence ---------------------------------------------------\n",
    "missing_items = [c for c in PHQ8_COLS if c not in labels_df.columns]\n",
    "if missing_items:\n",
    "    msg = f\"PHQ-8 schema mismatch: missing {len(missing_items)} column(s): {missing_items}\"\n",
    "    if REQUIRE_ALL_ITEMS:\n",
    "        raise AssertionError(msg)\n",
    "    else:\n",
    "        print(\"WARNING:\", msg, \" proceeding with available items only.\")\n",
    "        PHQ8_COLS = [c for c in PHQ8_COLS if c in labels_df.columns]\n",
    "\n",
    "if not PHQ8_COLS:\n",
    "    print(\"SKIP: No PHQ-8 item columns available; tabular features will be empty.\")\n",
    "    tab_df = pd.DataFrame(columns=[JOIN_KEY, TARGET])\n",
    "else:\n",
    "    # ---- 3.1.1 Assemble base frame ------------------------------------------\n",
    "    base_cols = [c for c in [JOIN_KEY, TARGET] if c in labels_df.columns]\n",
    "    tab_df = labels_df[base_cols + PHQ8_COLS].copy()\n",
    "\n",
    "    # Coerce items to numeric safely (handles stray strings gracefully)\n",
    "    items = tab_df[PHQ8_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # ---- 3.1.2 Clinical-style imputation & scoring ---------------------------\n",
    "    missing_ct = items.isna().sum(axis=1)        # items missing per row\n",
    "    row_mean   = items.mean(axis=1, skipna=True) # mean of answered items\n",
    "\n",
    "    # Impute only when exactly 1 (or 1) item missing\n",
    "    items_imputed = items.copy()\n",
    "    mask_impute = missing_ct.le(1) & missing_ct.gt(0)  # (0 < missing  1)\n",
    "    items_imputed.loc[mask_impute] = (\n",
    "        items_imputed.loc[mask_impute].T\n",
    "        .fillna(row_mean[mask_impute])  # broadcast row-wise means into NaNs\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    # Score:\n",
    "    #  - If 2 items missing  keep NaN (min_count enforces that)\n",
    "    #  - Else  sum imputed row\n",
    "    tab_df[\"phq8_missing_count\"] = missing_ct\n",
    "    tab_df[\"phq8_sum\"]  = items_imputed.sum(axis=1, min_count=len(PHQ8_COLS) - 1)\n",
    "    tab_df[\"phq8_mean\"] = items_imputed.mean(axis=1, skipna=True)\n",
    "\n",
    "    # ---- 3.1.3 Optional rounding to match reporting conventions -------------\n",
    "    if SCORE_ROUNDING == \"nearest\":\n",
    "        s = tab_df[\"phq8_sum\"]\n",
    "        tab_df[\"phq8_sum\"] = np.sign(s) * np.floor(np.abs(s) + 0.5)  # half-away-from-zero\n",
    "    elif SCORE_ROUNDING == \"bankers\":\n",
    "        tab_df[\"phq8_sum\"] = tab_df[\"phq8_sum\"].round(0)\n",
    "    elif SCORE_ROUNDING == \"floor\":\n",
    "        tab_df[\"phq8_sum\"] = np.floor(tab_df[\"phq8_sum\"])\n",
    "    elif SCORE_ROUNDING == \"ceil\":\n",
    "        tab_df[\"phq8_sum\"] = np.ceil(tab_df[\"phq8_sum\"])\n",
    "    # else: leave fractional totals as-is\n",
    "\n",
    "    # ---- 3.1.4 Post-scoring zero-fill for model inputs (documented choice) ---\n",
    "    # Keeps rows dense for models while preserving clinically faithful 'phq8_sum'.\n",
    "    tab_df[PHQ8_COLS] = items.fillna(0)\n",
    "\n",
    "    # ---- 3.1.5 Standardize numeric features (excluding target & ID) ----------\n",
    "    num_cols = [c for c in tab_df.columns\n",
    "                if c not in [JOIN_KEY, TARGET] and pd.api.types.is_numeric_dtype(tab_df[c])]\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        tab_df[[f\"{c}_z\" for c in num_cols]] = scaler.fit_transform(tab_df[num_cols])\n",
    "        print(f\"Scaled {len(num_cols)} numeric columns -> *_z\")\n",
    "    else:\n",
    "        print(\"NOTE: No numeric columns to scale.\")\n",
    "\n",
    "# ---- 3.1.6 Save & reviewer preview -----------------------------------------\n",
    "try:\n",
    "    tab_df.to_parquet(TAB_OUT, index=False)\n",
    "    print(\"Saved tabular PHQ-8 ->\", TAB_OUT, \"| shape=\", tab_df.shape)\n",
    "except Exception as e:\n",
    "    print(\"SKIP save:\", type(e).__name__, \"-\", e)\n",
    "\n",
    "show_cols = [JOIN_KEY, TARGET] + PHQ8_COLS + [\"phq8_missing_count\", \"phq8_sum\", \"phq8_mean\"]\n",
    "show_cols = [c for c in show_cols if c in tab_df.columns]\n",
    "print(\"tab_df preview:\")\n",
    "print(tab_df[show_cols].head(5))\n",
    "\n",
    "# ---- Optional QA against any provided 'phq8_score' column -------------------\n",
    "if \"phq8_score\" in labels_df.columns:\n",
    "    try:\n",
    "        orig = pd.to_numeric(labels_df[\"phq8_score\"], errors=\"coerce\")\n",
    "        agree = (orig.fillna(-1).astype(float) == tab_df[\"phq8_sum\"].fillna(-2).astype(float)).sum()\n",
    "        print(f\"QA: phq8_sum (clinical + rounding) vs phq8_score agreement: {agree}/{len(tab_df)} rows\")\n",
    "    except Exception:\n",
    "        print(\"QA: could not compare to 'phq8_score' (non-fatal).\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ•·ï¸ Spider Check - PHQ-8 Tabular Peek\n",
    "\n",
    "Anchoring in ground truth: survey responses and labels.  \n",
    "A simple check that PHQ-8 scores and symptom counts align with expectations.  \n",
    "\n",
    "***Because anchors keep us steady when we search for the unseen.***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHQ-8 QA (optional): compare our phq8_sum to provided phq8_score\n",
    "# =============================================================================\n",
    "if \"phq8_score\" in labels_df.columns and \"phq8_sum\" in tab_df.columns:\n",
    "    orig = pd.to_numeric(labels_df[\"phq8_score\"], errors=\"coerce\")\n",
    "    ours = pd.to_numeric(tab_df[\"phq8_sum\"], errors=\"coerce\")\n",
    "\n",
    "    mismask = orig.fillna(-1).astype(float) != ours.fillna(-2).astype(float)\n",
    "    mism_idx = mismask[mismask].index\n",
    "    n_mis = int(mismask.sum())\n",
    "\n",
    "    print(f\"QA: mismatches (ours vs provided): {n_mis}/{len(tab_df)} rows\")\n",
    "    if n_mis:\n",
    "        cols = [JOIN_KEY, \"phq8_score\", \"phq8_sum\", \"phq8_mean\", \"phq8_missing_count\"] + PHQ8_COLS\n",
    "        # Show up to 5 examples\n",
    "        preview = tab_df.loc[mism_idx, [c for c in cols if c in tab_df.columns]].head(5).copy()\n",
    "        # Add the provided score for clarity (from labels_df)\n",
    "        preview[\"phq8_score_src\"] = labels_df.loc[preview.index, \"phq8_score\"]\n",
    "        display(preview)\n",
    "else:\n",
    "    print(\"QA: skipped (no 'phq8_score' column or 'phq8_sum' not computed).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "### PHQ-8 tabular features: interpretation & key takeaways\n",
    "\n",
    "**What we did**\n",
    "- Pinned PHQ-8 item schema: [\"phq8_nointerest\",\"phq8_depressed\",\"phq8_sleep\",\"phq8_tired\",\"phq8_appetite\",\"phq8_failure\",\"phq8_concentrating\",\"phq8_moving\"].\n",
    "- Clinical-style scoring:\n",
    "  - If 1 item missing: imputed the missing item with the row mean of answered items, then summed.\n",
    "  - If 2 items missing: left the score as NaN (no aggressive imputation).\n",
    "- Optional rounding: set to \"nearest\" so totals match typical reporting.\n",
    "- After scoring, zero-filled item columns for modeling, and z-scored numeric features for comparability.\n",
    "\n",
    "**Guardrails & QA**\n",
    "- Label domain and split checks run in Step 2 (fail-fast or SKIP cleanly).\n",
    "- PHQ-8 QA: our computed \"phq8_sum\" vs provided \"phq8_score\"  **107/107** agreement with rounding (\"nearest\").\n",
    "\n",
    "**Results snapshot**\n",
    "- Saved to `data/processed/tabular_phq8.parquet`.\n",
    "- Shape: **(107, 24)** (ID, label, 8 items, missing_count, sum, mean, and z-scored variants).\n",
    "- Missingness: `phq8_missing_count` shows per-row item gaps; rows with 2 missing keep `phq8_sum` as NaN.\n",
    "\n",
    "**How to read the features**\n",
    "- `phq8_sum`: total symptom burden (higher = more severe).\n",
    "- `phq8_mean`: average per-item severity (robust when one item is imputed).\n",
    "- `phq8_missing_count`: data quality indicator; consider as a covariate or filter in sensitivity analyses.\n",
    "- `*_z`: standardized versions for models that benefit from scaled inputs.\n",
    "\n",
    "**Decisions (documented)**\n",
    "- Rounding: used \"nearest\" to mirror the provided clinical scores (prevents off-by-one drift when one item is imputed).\n",
    "- Post-scoring zero-fill: keeps downstream models dense without altering the clinically faithful `phq8_sum`.\n",
    "\n",
    "**Limitations**\n",
    "- Row-mean imputation for a single missing item is simple and standard, but still an assumption.\n",
    "- Rows with 2 missing items are not scored; downstream models should either ignore `phq8_sum` for those rows or handle NaNs explicitly.\n",
    "\n",
    "**Recommended next steps**\n",
    "- Sensitivity check: run models with and without rounding; confirm conclusions are stable.\n",
    "- Optionally add `phq8_flag_gt1_missing = 1{missing_count  2}` as an exclusion flag or covariate.\n",
    "- Proceed to 3.2 (Text) to add linguistic signals; the tabular block provides a solid baseline.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Text (transcripts embeddings)\n",
    "- Option A (quick baseline): TF IDF on transcript text. \n",
    "- Option B (semantic): sentence embeddings (e.g., SentenceTransformers).\n",
    "\n",
    "> Note: If running offline or with limited resources, prefer TF IDF first; swap in embeddings later.\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2a Import transcripts (DAIC/AVEC-style) and join into labels_df  - ROBUST\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "RAW_DIR = ROOT_DIR / \"data\"\n",
    "\n",
    "# Reuse join key safely (in case cells ran out of order)\n",
    "JOIN_KEY = globals().get(\"JOIN_KEY\", \"participant_id\")\n",
    "\n",
    "tx_files = list(RAW_DIR.rglob(\"*_TRANSCRIPT.csv\"))\n",
    "print(f\"Found {len(tx_files)} transcript file(s) under {RAW_DIR}\")\n",
    "\n",
    "def _participant_id_from_stem(stem: str) -> str:\n",
    "    m = re.match(r\"^(\\d+)\", stem)\n",
    "    return m.group(1) if m else stem\n",
    "\n",
    "def _read_transcript_csv(path: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Try several parsers to handle comma/tab and odd encodings.\"\"\"\n",
    "    for kwargs in (\n",
    "        {\"engine\": \"python\", \"sep\": None},     # sniff delimiter\n",
    "        {\"sep\": \"\\t\"},                         # tab-separated\n",
    "        {\"sep\": \",\"},                          # comma-separated\n",
    "        {\"engine\": \"python\", \"sep\": r\"\\s+\"},   # any whitespace\n",
    "    ):\n",
    "        for enc in (\"utf-8\", \"latin-1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "            except Exception:\n",
    "                continue\n",
    "    print(f\"SKIP: could not read {path.name} with common parsers\")\n",
    "    return None\n",
    "\n",
    "# preferred text/speaker header names (case-insensitive)\n",
    "TEXT_CANDIDATES    = [\"transcript\", \"value\", \"text\", \"utterance\", \"content\"]\n",
    "SPEAKER_CANDIDATES = [\"speaker\", \"speaker_id\"]\n",
    "\n",
    "rows = []\n",
    "for p in tx_files:\n",
    "    df = _read_transcript_csv(p)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"SKIP: empty or unreadable -> {p.name}\")\n",
    "        continue\n",
    "\n",
    "    # If pandas mis-parsed delimiter, you might see a single big column with brackets.\n",
    "    # Split that if needed (rare, but seen in weird exports).\n",
    "    if len(df.columns) == 1 and df.columns[0].strip().startswith(\"[\") and \",\" in df.columns[0]:\n",
    "        # Try to split header string into real columns\n",
    "        raw = df.columns[0]\n",
    "        cols = [c.strip(\" '\\\"\") for c in raw.strip(\"[]\").split(\",\")]\n",
    "        df = df.rename(columns={df.columns[0]: cols[0]})\n",
    "        # No rows to split; most of the time this case doesn't have usable data.\n",
    "        print(f\"SKIP: header looked bundled in {p.name} -> columns recovered: {cols}\")\n",
    "\n",
    "    lower_map = {str(c).lower(): c for c in df.columns}\n",
    "    text_col = next((lower_map[c] for c in TEXT_CANDIDATES if c in lower_map), None)\n",
    "    if text_col is None:\n",
    "        print(f\"SKIP: no recognizable text column in {p.name} (cols={list(df.columns)[:10]})\")\n",
    "        continue\n",
    "\n",
    "    speaker_col = next((lower_map[c] for c in SPEAKER_CANDIDATES if c in lower_map), None)\n",
    "    if speaker_col is not None:\n",
    "        keep = df[speaker_col].astype(str).str.lower().isin(\n",
    "            [\"participant\", \"p\", \"subject\", \"interviewee\", \"patient\"]\n",
    "        )\n",
    "        if keep.any():\n",
    "            df = df.loc[keep]\n",
    "\n",
    "    text = (\n",
    "        df[text_col]\n",
    "        .astype(str).fillna(\"\")\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "        .tolist()\n",
    "    )\n",
    "    transcript = \" \".join([t for t in text if t])\n",
    "\n",
    "    part_id = _participant_id_from_stem(p.stem)\n",
    "    rows.append({\"participant_id\": part_id, \"transcript\": transcript})\n",
    "\n",
    "tx_df = pd.DataFrame(rows)\n",
    "print(\"Built transcripts table:\", tx_df.shape)\n",
    "\n",
    "# Save unified for provenance\n",
    "TRANSCRIPTS_UNIFIED = PROCESSED_DIR / \"transcripts_unified.csv\"\n",
    "tx_df.to_csv(TRANSCRIPTS_UNIFIED, index=False)\n",
    "print(\"Wrote unified transcripts ->\", TRANSCRIPTS_UNIFIED)\n",
    "\n",
    "# Merge into labels_df on JOIN_KEY, with safe dtype casting only if the columns exist\n",
    "if JOIN_KEY in labels_df.columns and not tx_df.empty:\n",
    "    labels_df[JOIN_KEY] = labels_df[JOIN_KEY].astype(str)\n",
    "    tx_df[\"participant_id\"] = tx_df[\"participant_id\"].astype(str)\n",
    "    before_cols = labels_df.shape[1]\n",
    "    labels_df = labels_df.merge(\n",
    "        tx_df.rename(columns={\"participant_id\": JOIN_KEY}), on=JOIN_KEY, how=\"left\"\n",
    "    )\n",
    "    print(f\"Merged transcripts into labels_df: columns {before_cols} -> {labels_df.shape[1]}\")\n",
    "    print(\"labels_df now has 'transcript':\", \"transcript\" in labels_df.columns)\n",
    "elif JOIN_KEY not in labels_df.columns:\n",
    "    print(f\"SKIP merge: JOIN_KEY '{JOIN_KEY}' not present in labels_df.columns={list(labels_df.columns)[:10]}\")\n",
    "else:\n",
    "    print(\"No transcripts constructed; 3.2 will SKIP safely.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ•·ï¸ Spider Check - Text Meta Peek\n",
    "\n",
    "Word footprints: character counts, tokens, sentence lengths.  \n",
    "A tiny check that transcripts really hold the shape we expect before diving deeper.  \n",
    "\n",
    "***Because even footprints tell a story of the unseen.*** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect any transcript-like columns\n",
    "tx_like = [c for c in labels_df.columns if \"transcript\" in c.lower()]\n",
    "print(\"Transcript-like columns:\", tx_like)\n",
    "\n",
    "# Coalesce to a single 'transcript' column (handles _x/_y cases)\n",
    "if \"transcript\" not in labels_df.columns:\n",
    "    cand_x = next((c for c in tx_like if c.endswith(\"_x\")), None)\n",
    "    cand_y = next((c for c in tx_like if c.endswith(\"_y\")), None)\n",
    "    cand_plain = next((c for c in tx_like if c == \"transcript\"), None)\n",
    "\n",
    "    src = cand_plain or cand_x or cand_y\n",
    "    if src:\n",
    "        labels_df[\"transcript\"] = labels_df[src]\n",
    "        # drop the extra copies if present\n",
    "        for c in set(tx_like) - {\"transcript\"}:\n",
    "            labels_df.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Has 'transcript' now:\", \"transcript\" in labels_df.columns)\n",
    "print(\"Non-null transcripts:\", int(labels_df[\"transcript\"].notna().sum()) if \"transcript\" in labels_df.columns else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2 Text (transcripts) - TF-IDF baseline + lightweight QC (SKIP-safe)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Goal:\n",
    "#    Provide a fast, interpretable textual signal using TF-IDF on transcripts.\n",
    "#    Add simple QC features (length in chars/tokens, sentence count) to inspect data quality.\n",
    "#    Degrade gracefully when transcripts are not available (print + write placeholders).\n",
    "#    Save artifacts to processed/ for later multimodal joins.\n",
    "# Why:\n",
    "#    TF-IDF gives a transparent baseline before heavier embeddings.\n",
    "#    QC features help reviewers see whether text length/coverage varies by subject/split.\n",
    "# =============================================================================\n",
    "\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- 3.2.0 Output locations (consistent with 3.1/Section 5) -----------------\n",
    "TEXT_TFIDF_OUT = PROCESSED_DIR / \"text_tfidf.parquet\"   # JOIN_KEY + tfidf_* columns (dense float32)\n",
    "TEXT_META_OUT  = PROCESSED_DIR / \"text_meta.parquet\"    # JOIN_KEY + QC features\n",
    "TEXT_VOCAB_OUT = PROCESSED_DIR / \"text_tfidf_vocab.json\"\n",
    "\n",
    "# ---- 3.2.1 Vectorizer settings (balanced for speed + signal) ----------------\n",
    "TFIDF_MAX_FEATS = 2048        # cap features for speed & dimensionality control\n",
    "TFIDF_NGRAMS    = (1, 2)      # unigrams + bigrams capture short cues/phrases\n",
    "TFIDF_MIN_DF    = 2           # drop terms that appear in only one document\n",
    "\n",
    "# ---- 3.2.2 Find the transcript column in labels_df --------------------------\n",
    "# We prefer an explicit 'transcript' column, but accept common variants or\n",
    "# any column containing the word 'transcript' (case-insensitive).\n",
    "TRANSCRIPT_CANDIDATES = [\"transcript\", \"transcript_text\", \"text\", \"utterance\", \"asr_text\"]\n",
    "\n",
    "def _find_transcript_column(df: pd.DataFrame) -> str | None:\n",
    "    lower = {str(c).lower(): c for c in df.columns}\n",
    "    # exact matches first (more predictable)\n",
    "    for name in TRANSCRIPT_CANDIDATES:\n",
    "        if name in lower:\n",
    "            return lower[name]\n",
    "    # fallback: any column whose name contains 'transcript'\n",
    "    for k, orig in lower.items():\n",
    "        if \"transcript\" in k:\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "tx_col = _find_transcript_column(labels_df)\n",
    "\n",
    "if tx_col is None:\n",
    "    # No transcripts yet  write empty placeholders so later joins don't break\n",
    "    print(\"SKIP: No transcript column found. Looked for:\", TRANSCRIPT_CANDIDATES)\n",
    "    pd.DataFrame(columns=[JOIN_KEY]).to_parquet(TEXT_TFIDF_OUT, index=False)\n",
    "    pd.DataFrame(columns=[JOIN_KEY]).to_parquet(TEXT_META_OUT, index=False)\n",
    "    Path(TEXT_VOCAB_OUT).write_text(json.dumps({\"vocab\": [], \"ngram_range\": TFIDF_NGRAMS, \"min_df\": TFIDF_MIN_DF}))\n",
    "else:\n",
    "    # ---- 3.2.3 Assemble/normalize text frame --------------------------------\n",
    "    # Keep only ID, target, and the transcript column (keeps artifacts small & deterministic)\n",
    "    base_cols = [c for c in [JOIN_KEY, TARGET, tx_col] if c in labels_df.columns]\n",
    "    text_df = labels_df[base_cols].copy()\n",
    "\n",
    "    # Normalize text lightly (stable across platforms; preserves punctuation for tokens)\n",
    "    text_df[tx_col] = (\n",
    "        text_df[tx_col].astype(\"string\").fillna(\"\")\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ---- 3.2.4 QC features (length in chars/tokens, sentence count) ----------\n",
    "    # Why: quick sanity to spot empty/short transcripts or outliers by subject/split.\n",
    "    def _tokenize(s: str) -> list[str]:\n",
    "        return re.findall(r\"\\b[\\w'-]+\\b\", s.lower())\n",
    "\n",
    "    meta = text_df[[JOIN_KEY]].copy()\n",
    "    meta[\"text_len_chars\"]      = text_df[tx_col].str.len().astype(\"Int64\")\n",
    "    meta[\"text_len_tokens\"]     = text_df[tx_col].apply(lambda s: len(_tokenize(s))).astype(\"Int64\")\n",
    "    meta[\"text_num_sentences\"]  = text_df[tx_col].str.count(r\"[.!?]\").astype(\"Int64\")\n",
    "    meta.to_parquet(TEXT_META_OUT, index=False)\n",
    "    print(f\"Saved text meta -> {TEXT_META_OUT} | shape={meta.shape}\")\n",
    "\n",
    "    # ---- 3.2.5 TF-IDF construction (only for nonempty transcripts) ----------\n",
    "    nonempty = text_df[text_df[tx_col].str.len() > 0]\n",
    "    if nonempty.empty:\n",
    "        print(f\"SKIP: transcript column '{tx_col}' is present but all rows are empty; TF-IDF not built.\")\n",
    "        pd.DataFrame(columns=[JOIN_KEY]).to_parquet(TEXT_TFIDF_OUT, index=False)\n",
    "        Path(TEXT_VOCAB_OUT).write_text(json.dumps({\"vocab\": [], \"ngram_range\": TFIDF_NGRAMS, \"min_df\": TFIDF_MIN_DF}))\n",
    "    else:\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        except Exception as e:\n",
    "            # Keep notebook runnable even if sklearn isn't installed\n",
    "            print(\"SKIP: scikit-learn not available for TF-IDF:\", type(e).__name__, \"-\", e)\n",
    "            pd.DataFrame(columns=[JOIN_KEY]).to_parquet(TEXT_TFIDF_OUT, index=False)\n",
    "            Path(TEXT_VOCAB_OUT).write_text(json.dumps({\"vocab\": [], \"ngram_range\": TFIDF_NGRAMS, \"min_df\": TFIDF_MIN_DF}))\n",
    "        else:\n",
    "            vec = TfidfVectorizer(\n",
    "                max_features=TFIDF_MAX_FEATS,\n",
    "                ngram_range=TFIDF_NGRAMS,\n",
    "                min_df=TFIDF_MIN_DF,\n",
    "                stop_words=\"english\",\n",
    "                strip_accents=\"unicode\",\n",
    "                dtype=np.float32,\n",
    "\n",
    "            )\n",
    "            X = vec.fit_transform(nonempty[tx_col].tolist())\n",
    "            vocab = vec.get_feature_names_out().tolist()\n",
    "            tfidf_cols = [f\"tfidf_{t}\" for t in vocab]\n",
    "\n",
    "            # Parquet does not support pandas Sparse by default  densify to float32\n",
    "            arr = X.toarray().astype(\"float32\")\n",
    "            tfidf_df = pd.DataFrame(arr, columns=tfidf_cols)\n",
    "            tfidf_df.insert(0, JOIN_KEY, nonempty[JOIN_KEY].to_numpy())\n",
    "\n",
    "            # Save compressed parquet + sidecar vocab (for reproducibility)\n",
    "            tfidf_df.to_parquet(TEXT_TFIDF_OUT, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "            Path(TEXT_VOCAB_OUT).write_text(json.dumps(\n",
    "                {\"vocab\": vocab, \"ngram_range\": TFIDF_NGRAMS, \"min_df\": TFIDF_MIN_DF}\n",
    "            ))\n",
    "            print(f\"Saved TF-IDF -> {TEXT_TFIDF_OUT} | shape={tfidf_df.shape}\")\n",
    "            print(f\"Saved TF-IDF vocab -> {TEXT_VOCAB_OUT} | {len(vocab)} terms\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in labels_df.columns if any(k in str(c).lower() for k in [\"transcript\",\"text\",\"utter\",\"asr\",\"notes\",\"summary\",\"content\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ•·ï¸ Spider Check - TF-IDF Sanity Peek\n",
    "\n",
    "Before building further, I pause for a **Spider Check**:  \n",
    "like pulling back the covers in a cabin to make sure there are no critters,  \n",
    "I peek into my data with a quick `head(2)` or shape check.  \n",
    "\n",
    "It's not just a sanity step - it's a peace-of-mind ritual.  \n",
    "***Because good data science isn't just about seeing what's obvious -  \n",
    "it's about seeing the unseen.***\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2b Spider check - TF-IDF IDF transparency peek (read-only)\n",
    "# -----------------------------------------------------------------------------\n",
    "# What this does:\n",
    "#    Reports how many participants have non-empty transcripts\n",
    "#    Re-fits a TF-IDF vectorizer (same settings) purely to read IDF weights\n",
    "#    Prints top/bottom IDF terms (rarest/most common)\n",
    "#    Flags \"spidery\" terms (very short or odd chars) as a quick noise check\n",
    "#    Saves a CSV of all terms+IDF for provenance: data/processed/text_idf_terms.csv\n",
    "# Why separate from 3.2:\n",
    "#    Keeps feature-building cell clean; this is an inspection-only \"peek\"\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Guard: do we even have transcripts merged yet?\n",
    "if \"transcript\" not in labels_df.columns:\n",
    "    print(\"No 'transcript' column in labels_df; nothing to peek.\")\n",
    "else:\n",
    "    # 1) Coverage summary (how many non-empty transcripts do we actually have?)\n",
    "    tx_series = labels_df[\"transcript\"].astype(str)\n",
    "    nonempty_mask = tx_series.str.len() > 0\n",
    "    tx = tx_series[nonempty_mask].tolist()\n",
    "    print(f\"Non-empty transcripts: {nonempty_mask.sum()} / {len(labels_df)}\")\n",
    "\n",
    "    if not tx:\n",
    "        print(\"All transcripts are empty - skipping IDF peek.\")\n",
    "    else:\n",
    "        # 2) Refit a tiny TF-IDF with the SAME settings used in 3.2\n",
    "        #    (We only need the fitted vocabulary + IDF weights for transparency.)\n",
    "        vec = TfidfVectorizer(\n",
    "            max_features=TFIDF_MAX_FEATS,\n",
    "            ngram_range=TFIDF_NGRAMS,\n",
    "            min_df=TFIDF_MIN_DF,\n",
    "            stop_words=\"english\",\n",
    "            strip_accents=\"unicode\",\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        X = vec.fit_transform(tx)                         # fit only on non-empty docs\n",
    "        vocab = vec.get_feature_names_out().tolist()      # learned terms (size <= max_features)\n",
    "        idf_vals = vec.idf_.astype(float)                 # IDF weights; higher = rarer\n",
    "\n",
    "        idf_table = (\n",
    "            pd.DataFrame({\"term\": vocab, \"idf\": idf_vals})\n",
    "            .sort_values(\"idf\", ascending=False)          # rarest first\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # 3) Show a small slice for reviewers\n",
    "        print(\"\\nTop 15 highest-IDF (rarest) terms:\")\n",
    "        print(idf_table.head(15).to_string(index=False))\n",
    "\n",
    "        print(\"\\nBottom 15 lowest-IDF (most common) terms:\")\n",
    "        print(idf_table.tail(15).to_string(index=False))\n",
    "\n",
    "        # 4) \"Spidery\" quick check (very short tokens or tokens with odd characters)\n",
    "        spidery_mask = idf_table[\"term\"].str.match(r\"(^.{,2}$|.*[^a-z0-9_'\\-].*)\", case=False)\n",
    "        spidery = idf_table[spidery_mask]\n",
    "        if not spidery.empty:\n",
    "            print(f\"\\nHeads-up: {len(spidery)} suspicious terms (very short or odd chars).\")\n",
    "            print(spidery.head(10).to_string(index=False))\n",
    "\n",
    "        # 5) Save full IDF table for provenance\n",
    "        TEXT_IDF_CSV = PROCESSED_DIR / \"text_idf_terms.csv\"\n",
    "        idf_table.to_csv(TEXT_IDF_CSV, index=False)\n",
    "        print(\"\\nSaved IDF table ->\", TEXT_IDF_CSV, \"| shape =\", idf_table.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "**How to read this (TF-IDF IDF transparency)**\n",
    "- **High IDF** = rarer terms  often more distinctive for a document; review to ensure no leaky or sensitive tokens.\n",
    "- **Low IDF** = very common terms  candidates for stoplist if they're uninformative in this corpus.\n",
    "- **Spidery terms** (very short / odd characters)  likely typos, artifacts, or noise; consider cleaning or adding to a custom stoplist.\n",
    "- We saved `data/processed/text_idf_terms.csv` so others can audit terms across runs.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2c Custom-stoplist TF-IDF (parallel artifacts for comparison)\n",
    "# -----------------------------------------------------------------------------\n",
    "# What:\n",
    "#    Adds conversational fillers & obvious noise to a custom stoplist\n",
    "#    Normalizes stopwords with the SAME analyzer TF-IDF uses (no warnings)\n",
    "#    Rebuilds TF-IDF (same settings as 3.2) and saves parallel artifacts\n",
    "# Why:\n",
    "#    Remove uninformative speech tokens; keep features focused on content\n",
    "#    Side-by-side artifacts let us compare base vs custom stoplists\n",
    "# =============================================================================\n",
    "\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Guard\n",
    "if \"transcript\" not in labels_df.columns:\n",
    "    print(\"No 'transcript' column available; skipping custom TF-IDF.\")\n",
    "else:\n",
    "    tx_series = labels_df[\"transcript\"].astype(str)\n",
    "    nonempty_mask = tx_series.str.len() > 0\n",
    "    nonempty = tx_series[nonempty_mask].tolist()\n",
    "    if not nonempty:\n",
    "        print(\"All transcripts empty; skipping custom TF-IDF.\")\n",
    "    else:\n",
    "        # --- 1) Conversational fillers / noises to filter (extend as needed)\n",
    "        CUSTOM_STOPS = {\n",
    "            \"um\",\"umm\",\"uh\",\"uhh\",\"uhhh\",\"ah\",\"oh\",\"hmm\",\"hmmm\",\"mmm\",\n",
    "            \"yeah\",\"yep\",\"nope\",\"ok\",\"okay\",\n",
    "            \"like\",\"just\",\"really\",\"kinda\",\"sorta\",\"ya\",\"yall\",\"y'all\",\n",
    "            \"you\",\"youre\",\"you're\",\"youknow\",\"ya know\",\"you know\",\n",
    "            \"i\",\"im\",\"i'm\",\"id\",\"i'd\",\"ive\",\"i've\",\"me\",\"my\",\"mine\",\n",
    "            \"uh-huh\",\"huh\",\"mm-hmm\",\n",
    "            \"laughter\"  # appears very frequently in this corpus\n",
    "        }\n",
    "\n",
    "        # --- 2) Build a temporary vectorizer to get the SAME analyzer as our TF-IDF\n",
    "        _tmp_vec = TfidfVectorizer(\n",
    "            ngram_range=TFIDF_NGRAMS,\n",
    "            min_df=TFIDF_MIN_DF,\n",
    "            stop_words=None,            # no stops yet\n",
    "            strip_accents=\"unicode\",\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        analyzer = _tmp_vec.build_analyzer()\n",
    "\n",
    "        # Merge sklearn's English stops + our conversational stops, then NORMALIZE with the analyzer\n",
    "        custom_only   = {s.lower() for s in CUSTOM_STOPS}\n",
    "        raw_stopset   = set(ENGLISH_STOP_WORDS) | custom_only\n",
    "\n",
    "        norm_stopset = set()\n",
    "        for s in raw_stopset:\n",
    "            # e.g., \"I've\" -> [\"ve\"], \"mm-hmm\" -> [\"mm\",\"hmm\"]\n",
    "            for tok in analyzer(s):\n",
    "                norm_stopset.add(tok)\n",
    "\n",
    "        STOPLIST = sorted(norm_stopset)  # list-like, deterministic\n",
    "        added_beyond_english = len(custom_only - set(ENGLISH_STOP_WORDS))\n",
    "        print(f\"Custom stoplist (normalized) size: {len(STOPLIST)} (added {added_beyond_english} beyond sklearn English)\")\n",
    "\n",
    "        # --- 3) Vectorize with the SAME settings as 3.2, but using our STOPLIST\n",
    "        vec_custom = TfidfVectorizer(\n",
    "            max_features=TFIDF_MAX_FEATS,\n",
    "            ngram_range=TFIDF_NGRAMS,\n",
    "            min_df=TFIDF_MIN_DF,\n",
    "            stop_words=STOPLIST,\n",
    "            strip_accents=\"unicode\",\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        Xc = vec_custom.fit_transform(nonempty)\n",
    "        vocab_c = vec_custom.get_feature_names_out().tolist()\n",
    "\n",
    "        # --- 4) Dense float32 for Parquet\n",
    "        arr = Xc.toarray().astype(\"float32\")\n",
    "        tfidf_cols_c = [f\"tfidf_{t}\" for t in vocab_c]\n",
    "        tfidf_custom = pd.DataFrame(arr, columns=tfidf_cols_c)\n",
    "\n",
    "        # Align IDs to the same non-empty mask used above\n",
    "        ids_nonempty = labels_df.loc[nonempty_mask, JOIN_KEY].to_numpy()\n",
    "        tfidf_custom.insert(0, JOIN_KEY, ids_nonempty)\n",
    "\n",
    "        # --- 5) Save parallel artifacts\n",
    "        TEXT_TFIDF_CUSTOM = PROCESSED_DIR / \"text_tfidf_custom.parquet\"\n",
    "        TEXT_VOCAB_CUSTOM = PROCESSED_DIR / \"text_tfidf_vocab_custom.json\"\n",
    "        TEXT_IDF_CUSTOM   = PROCESSED_DIR / \"text_idf_terms_custom.csv\"\n",
    "\n",
    "        tfidf_custom.to_parquet(TEXT_TFIDF_CUSTOM, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        Path(TEXT_VOCAB_CUSTOM).write_text(json.dumps(\n",
    "            {\n",
    "                \"vocab\": vocab_c,\n",
    "                \"ngram_range\": TFIDF_NGRAMS,\n",
    "                \"min_df\": TFIDF_MIN_DF,\n",
    "                \"custom_stops\": sorted(list(CUSTOM_STOPS)),  # human-readable list we started with\n",
    "            }\n",
    "        ))\n",
    "\n",
    "        idf_c = vec_custom.idf_.astype(float)\n",
    "        idf_table_c = pd.DataFrame({\"term\": vocab_c, \"idf\": idf_c}).sort_values(\"idf\", ascending=False)\n",
    "        idf_table_c.to_csv(TEXT_IDF_CUSTOM, index=False)\n",
    "\n",
    "        print(f\"Saved TF-IDF (custom) -> {TEXT_TFIDF_CUSTOM} | shape={tfidf_custom.shape}\")\n",
    "        print(f\"Saved TF-IDF vocab (custom) -> {TEXT_VOCAB_CUSTOM} | {len(vocab_c)} terms\")\n",
    "        print(f\"Saved IDF table (custom) -> {TEXT_IDF_CUSTOM} | shape={idf_table_c.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2d Visual: Top-IDF terms (base vs custom)\n",
    "Quick look at the rarest (highest-IDF) terms under the baseline and custom-stoplist runs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2d Visual: top-IDF bars (base vs custom)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "base_idf = PROCESSED_DIR / \"text_idf_terms.csv\"\n",
    "cust_idf = PROCESSED_DIR / \"text_idf_terms_custom.csv\"\n",
    "\n",
    "if base_idf.exists() and cust_idf.exists():\n",
    "    base_df = pd.read_csv(base_idf).sort_values(\"idf\", ascending=False).head(15)\n",
    "    cust_df = pd.read_csv(cust_idf).sort_values(\"idf\", ascending=False).head(15)\n",
    "\n",
    "    # Base (top 15 rarest)\n",
    "    ax = base_df.sort_values(\"idf\").plot(kind=\"barh\", x=\"term\", y=\"idf\", legend=False)\n",
    "    ax.set_title(\"Top 15 highest-IDF terms - BASE\")\n",
    "    ax.set_xlabel(\"IDF\"); ax.set_ylabel(\"term\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Custom (top 15 rarest)\n",
    "    plt.figure()\n",
    "    ax = cust_df.sort_values(\"idf\").plot(kind=\"barh\", x=\"term\", y=\"idf\", legend=False)\n",
    "    ax.set_title(\"Top 15 highest-IDF terms - CUSTOM stoplist\")\n",
    "    ax.set_xlabel(\"IDF\"); ax.set_ylabel(\"term\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    base_terms   = set(base_df[\"term\"])\n",
    "    custom_terms = set(cust_df[\"term\"])\n",
    "    print(\"Rarest terms unique to BASE:  \", sorted(base_terms - custom_terms)[:10])\n",
    "    print(\"Rarest terms unique to CUSTOM:\", sorted(custom_terms - base_terms)[:10])\n",
    "else:\n",
    "    print(\"Missing one of:\", base_idf, cust_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2e Visual: Transcript length distribution (QC)\n",
    "How many tokens per transcript? Outliers or very short transcripts stand out here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2e Visual: histogram of transcript token lengths\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "meta_path = PROCESSED_DIR / \"text_meta.parquet\"\n",
    "if meta_path.exists():\n",
    "    meta = pd.read_parquet(meta_path)\n",
    "    ax = meta[\"text_len_tokens\"].dropna().plot(kind=\"hist\", bins=20)\n",
    "    ax.set_title(\"Transcript token length distribution\")\n",
    "    ax.set_xlabel(\"tokens per transcript\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(\"Summary stats:\")\n",
    "    print(meta[\"text_len_tokens\"].describe())\n",
    "else:\n",
    "    print(\"Missing:\", meta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2f Visual: PHQ-8 vs transcript length\n",
    "Sanity correlation (not causal): do longer transcripts co-vary with PHQ-8?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2f Visual: PHQ-8 vs transcript length scatter with Pearson r\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "tab_path  = PROCESSED_DIR / \"tabular_phq8.parquet\"\n",
    "meta_path = PROCESSED_DIR / \"text_meta.parquet\"\n",
    "\n",
    "if tab_path.exists() and meta_path.exists():\n",
    "    phq  = pd.read_parquet(tab_path)[[JOIN_KEY, \"phq8_sum\"]]\n",
    "    meta = pd.read_parquet(meta_path)[[JOIN_KEY, \"text_len_tokens\"]]\n",
    "    merged = phq.merge(meta, on=JOIN_KEY, how=\"inner\").dropna()\n",
    "\n",
    "    if len(merged) >= 10:\n",
    "        r, p = pearsonr(merged[\"phq8_sum\"].astype(float), merged[\"text_len_tokens\"].astype(float))\n",
    "        ax = merged.plot(kind=\"scatter\", x=\"text_len_tokens\", y=\"phq8_sum\", alpha=0.6)\n",
    "        ax.set_title(f\"PHQ-8 vs Transcript Length  (r={r:.3f}, p={p:.3g}, n={len(merged)})\")\n",
    "        ax.set_xlabel(\"tokens per transcript\"); ax.set_ylabel(\"PHQ-8 sum\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        print(merged[[JOIN_KEY, \"phq8_sum\", \"text_len_tokens\"]].head(5))\n",
    "    else:\n",
    "        print(\"Not enough rows for correlation (n<10). Current n =\", len(merged))\n",
    "else:\n",
    "    print(\"Missing one of:\", tab_path, meta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2g Visual: Vocab overlap (base vs custom)\n",
    "How much of the vocabulary is shared vs unique across the two runs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2g Visual: base vs custom vocab overlap\n",
    "import json, matplotlib.pyplot as plt\n",
    "\n",
    "base_meta_path   = PROCESSED_DIR / \"text_tfidf_vocab.json\"\n",
    "custom_meta_path = PROCESSED_DIR / \"text_tfidf_vocab_custom.json\"\n",
    "\n",
    "if base_meta_path.exists() and custom_meta_path.exists():\n",
    "    base_meta   = json.loads(base_meta_path.read_text())\n",
    "    custom_meta = json.loads(custom_meta_path.read_text())\n",
    "    base_terms   = set(base_meta.get(\"vocab\", []))\n",
    "    custom_terms = set(custom_meta.get(\"vocab\", []))\n",
    "\n",
    "    only_base   = len(base_terms - custom_terms)\n",
    "    only_custom = len(custom_terms - base_terms)\n",
    "    both        = len(base_terms & custom_terms)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar([\"base only\", \"both\", \"custom only\"], [only_base, both, only_custom])\n",
    "    plt.title(\"Vocab overlap: base vs custom stoplist\")\n",
    "    plt.ylabel(\"terms\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f\"base |V|={len(base_terms)}, custom |V|={len(custom_terms)}, both={both}\")\n",
    "else:\n",
    "    print(\"Missing one of:\", base_meta_path, custom_meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Narrative - What was done & why\n",
    "\n",
    "**Goal.** Build transparent text features from interview transcripts that we can audit and reproduce.\n",
    "\n",
    "**What we did.**\n",
    "- **3.2a Import & join.** Discovered per-participant `*_TRANSCRIPT.csv` files, concatenated a participant's utterances  one `transcript` string per `participant_id`, and merged into `labels_df`.\n",
    "- **3.2 TF-IDF baseline + QC.** \n",
    "  - Cleaned text lightly (whitespace/accents).\n",
    "  - Built **TF-IDF** with `max_features=2048`, `ngram_range=(1,2)`, `min_df=2`, English stopwords; saved:\n",
    "    - `text_tfidf.parquet` (dense float32 features) and `text_tfidf_vocab.json` (vocabulary + settings).\n",
    "  - Wrote **QC features** per participant (`text_len_chars`, `text_len_tokens`, `text_num_sentences`) to `text_meta.parquet`.\n",
    "- **3.2c Spider check (transparency).** Refit TF-IDF only to read the **IDF table** (rare  common terms), flagged \"spidery\" tokens (very short/odd), and saved `text_idf_terms.csv` for provenance.\n",
    "- **3.2d Custom stoplist.** Added a small set of conversational fillers and obvious noise (um/uh/like/just/...); **normalized** the stoplist using the same analyzer TF-IDF uses (no inconsistency warnings). Rebuilt a parallel TF-IDF and saved:\n",
    "  - `text_tfidf_custom.parquet`, `text_tfidf_vocab_custom.json`, `text_idf_terms_custom.csv`.\n",
    "- **Section 4 merge.** Assembled a **multimodal** table with PHQ-8 (`tab_*`), text QC (`txt_*`), base TF-IDF (`tfidf_*`), and custom TF-IDF (`tfidfC_*`).\n",
    "\n",
    "**Why this design.**\n",
    "- **Transparency:** IDF tables + QC stats make structure visible to reviewers.\n",
    "- **Reproducibility:** sidecar vocab JSONs and saved IDF CSVs document exactly what was used.\n",
    "- **Graceful failure:** SKIP-safe cells keep nbconvert/CI green even if a modality isn't ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Results & Key Takeaways\n",
    "\n",
    "**Coverage & QC.**\n",
    "- **Non-empty transcripts:** 110 / 110 (in the spider-check set).\n",
    "- **Transcript length (tokens):** mean  **1410**, std  **787**, min **78**, max **4116**. Most interviews fall in a mid-length band; very short transcripts are potential QC outliers.\n",
    "- **PHQ-8 vs length:** Pearson **r = 0.103**, **p = 0.283**, **n = 110**  weak, non-significant association (descriptive only, not causal).\n",
    "\n",
    "**Vocabulary & IDF.**\n",
    "- **Baseline TF-IDF:** 2048 terms (cap), unigrams+bigrams with English stopwords.\n",
    "- **Custom stoplist TF-IDF:** 2048 terms (cap) after removing fillers (um/uh/like/just/...).\n",
    "- **Overlap:** **both  1355** terms; **base-only  693**, **custom-only  693**  the custom list trims conversational filler without collapsing the feature space.\n",
    "- **Rarest (high-IDF) terms:** surface topical words (examples from this run included items like \"coughs\", \"mountain\", \"marketing\", \"prison\", etc.); good places to check for typos/sensitive tokens.\n",
    "- **Most common (low-IDF) terms:** generic/filler (\"things\", \"laughter\", \"think\", \"people\", \"know\", \"really\", \"like\", \"um/uh/just\"); strong candidates for stoplisting.\n",
    "\n",
    "**What changed with the custom stoplist.**\n",
    "- Frequent filler tokens were removed/discounted; bar charts show rarer **topical** terms move up the IDF ranks.\n",
    "- Vocab **stability** remained high ( two-thirds shared), indicating a modest, targeted refocus rather than a wholesale shift.\n",
    "\n",
    "**Artifacts (reproducible).**\n",
    "- Baseline: `text_meta.parquet`, `text_tfidf.parquet`, `text_tfidf_vocab.json`, `text_idf_terms.csv`\n",
    "- Custom: `text_tfidf_custom.parquet`, `text_tfidf_vocab_custom.json`, `text_idf_terms_custom.csv`\n",
    "- Merged: `multimodal_features.parquet` with `tab_*`, `txt_*`, `tfidf_*`, `tfidfC_*`\n",
    "\n",
    "**Limitations / notes.**\n",
    "- TF-IDF is bag-of-words; no syntax/semantics. We'll consider **sentence embeddings** later if resources allow.\n",
    "- Stoplists reduce noise but are corpus-specific; we'll keep an eye on \"spidery\" terms and adjust iteratively.\n",
    "\n",
    "**Next steps.**\n",
    "- (A) Optional: refine custom stops (e.g., domain-specific fillers), re-run 3.2d and compare IDF tables.\n",
    "- (B) Proceed to **3.3 Audio** (prosody aggregates) and **3.4 Video** (AU/gaze aggregates).\n",
    "- (C) Train quick baselines on text-only (TF-IDF logistic) vs tabular-only vs multimodal to quantify lift.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 3.2 Executive Summary (Text Features)\n",
    "\n",
    "We ingested and joined per-participant transcripts, then built a transparent TF-IDF baseline (unigram+bigram, 2K cap) with QC stats; coverage is 110/110 non-empty transcripts. A custom stoplist (normalized to the analyzer) removed conversational fillers (um/uh/like/just/...), yielding a parallel TF-IDF that preserved vocabulary stability (1,355 shared terms; balanced base-only vs custom-only) while surfacing more topical rare terms. Descriptively, transcript length showed a weak, non-significant association with PHQ-8 (r  0.10, p  0.28), suggesting verbosity isn't tightly linked to severity in this set. All artifacts (vocab JSONs, IDF tables, features) are saved for reproducibility, and both BASE (`tfidf_*`) and CUSTOM (`tfidfC_*`) features are included in the merged multimodal table for downstream modeling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIAGNOSTIC: find a root, list counts, show sample columns (audio + video) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Confirm ROOT_DIR (used everywhere in NB03)\n",
    "try:\n",
    "    print(\"ROOT_DIR =\", ROOT_DIR)\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().resolve().parent\n",
    "    print(\"ROOT_DIR not set; using parent of notebooks/:\", ROOT_DIR)\n",
    "\n",
    "# 1) Pick a search root (prefer folders you actually showed in screenshots)\n",
    "CAND_ROOTS = [\n",
    "    ROOT_DIR / \"data\" / \"daic_woz\",\n",
    "    ROOT_DIR / \"data\" / \"raw\" / \"daic_woz\",\n",
    "    ROOT_DIR / \"data\" / \"raw\",\n",
    "    ROOT_DIR / \"data\",\n",
    "]\n",
    "\n",
    "ROOT = next((r for r in CAND_ROOTS if r.exists()), ROOT_DIR / \"data\")\n",
    "print(\"Search ROOT:\", ROOT)\n",
    "\n",
    "# 2) Collect matches (don't print paths yet; just counts)\n",
    "aud_cov   = list(ROOT.rglob(\"*COVAREP.csv\"))\n",
    "aud_form  = list(ROOT.rglob(\"*FORMANT.csv\"))\n",
    "vid_clnf  = list(ROOT.rglob(\"*CLNF_features.*\"))\n",
    "vid_openf = list(ROOT.rglob(\"*OpenFace*.csv\"))\n",
    "vid_aus   = list(ROOT.rglob(\"*_AUs.csv\"))\n",
    "\n",
    "print(\"\\nCounts:\")\n",
    "print(\" audio COVAREP:\", len(aud_cov))\n",
    "print(\" audio FORMANT:\", len(aud_form))\n",
    "print(\" video CLNF_features.*:\", len(vid_clnf))\n",
    "print(\" video OpenFace*.csv:\", len(vid_openf))\n",
    "print(\" video *_AUs.csv:\", len(vid_aus))\n",
    "\n",
    "# 3) Show 2 audio + 2 video examples (paths + first 12 columns)\n",
    "def _peek_csv(path: Path):\n",
    "    for kwargs in ({\"engine\":\"python\",\"sep\":None},{\"sep\":\"\\t\"},{\"sep\":\",\"},{\"engine\":\"python\",\"sep\":r\"\\s+\"}):\n",
    "        for enc in (\"utf-8\",\"latin-1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, **kwargs, nrows=3)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "print(\"\\nSample audio files:\")\n",
    "for p in (aud_cov[:1] + aud_form[:1]):\n",
    "    print(\" \", p)\n",
    "    df = _peek_csv(p)\n",
    "    print(\"   cols:\", list(df.columns)[:12] if df is not None else \"unreadable\")\n",
    "\n",
    "print(\"\\nSample video files:\")\n",
    "for p in (vid_clnf[:1] + vid_openf[:1] + vid_aus[:1])[:2]:\n",
    "    print(\" \", p)\n",
    "    df = _peek_csv(p)\n",
    "    print(\"   cols:\", list(df.columns)[:12] if df is not None else \"unreadable\")\n",
    "\n",
    "print(\"\\n(If these are all zero, verify the folder path in CAND_ROOTS matches where your files live.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3 Audio (prosody)\n",
    "- Fundamental frequency (f0), jitter/shimmer, loudness/energy, spectral features. \n",
    "- Extract with OpenSMILE or COVAREP, then aggregate per session (mean, std, percentiles).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.3 Audio (prosody) - per-participant aggregates (robust, SKIP-safe, verbose)\n",
    "# =============================================================================\n",
    "import sys, time, re, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "AUDIO_OUT = PROCESSED_DIR / \"audio_features.parquet\"\n",
    "\n",
    "# --- Verbosity controls ---\n",
    "VERBOSE_EVERY = 25   # print a line every N files\n",
    "LIMIT = None         # set to e.g. 30 for a quick dry-run, then None for full run\n",
    "\n",
    "# Candidate roots (pick first that exists)\n",
    "CAND_ROOTS = [\n",
    "    ROOT_DIR / \"data\" / \"daic_woz\",\n",
    "    ROOT_DIR / \"data\" / \"raw\" / \"daic_woz\",\n",
    "    ROOT_DIR / \"data\" / \"raw\",\n",
    "    ROOT_DIR / \"data\",\n",
    "]\n",
    "RAW_DIR = next((r for r in CAND_ROOTS if r.exists()), ROOT_DIR / \"data\")\n",
    "print(\"Audio search ROOT:\", RAW_DIR)\n",
    "\n",
    "def _pid_from_stem(stem: str) -> str:\n",
    "    m = re.match(r\"^(\\d+)\", stem)\n",
    "    return m.group(1) if m else stem\n",
    "\n",
    "def _read_csv_any(path: Path) -> pd.DataFrame | None:\n",
    "    for kwargs in ({\"engine\":\"python\",\"sep\":None},{\"sep\":\"\\t\"},{\"sep\":\",\"},{\"engine\":\"python\",\"sep\":r\"\\s+\"}):\n",
    "        for enc in (\"utf-8\",\"latin-1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, **kwargs, low_memory=False)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "audio_files = list(RAW_DIR.rglob(\"*COVAREP.csv\")) + list(RAW_DIR.rglob(\"*FORMANT.csv\"))\n",
    "print(f\"Found {len(audio_files)} audio feature file(s)\")\n",
    "if LIMIT:\n",
    "    audio_files = audio_files[:LIMIT]\n",
    "    print(f\"DRY-RUN: limiting to first {len(audio_files)} files\")\n",
    "\n",
    "rows, t0 = [], time.time()\n",
    "for i, p in enumerate(audio_files, 1):\n",
    "    df = _read_csv_any(p)\n",
    "    if df is None or df.empty:\n",
    "        print(\"SKIP: unreadable/empty audio file ->\", p.name); continue\n",
    "\n",
    "    # Coerce numbers robustly\n",
    "    num = df.apply(pd.to_numeric, errors=\"coerce\").select_dtypes(include=[np.number])\n",
    "    if num.empty:\n",
    "        print(\"SKIP: no numeric columns in\", p.name); continue\n",
    "\n",
    "    part = _pid_from_stem(p.stem)\n",
    "    src  = \"COVAREP\" if \"COVAREP\" in p.name.upper() else \"FORMANT\"\n",
    "    agg = {\n",
    "        **num.mean(numeric_only=True).add_prefix(\"mean_\").to_dict(),\n",
    "        **num.std(numeric_only=True).add_prefix(\"std_\").to_dict(),\n",
    "        **num.median(numeric_only=True).add_prefix(\"med_\").to_dict(),\n",
    "        **num.quantile(0.10, numeric_only=True).add_prefix(\"p10_\").to_dict(),\n",
    "        **num.quantile(0.90, numeric_only=True).add_prefix(\"p90_\").to_dict(),\n",
    "        \"n_frames\": len(num),\n",
    "        \"source\": src,\n",
    "    }\n",
    "    rows.append({\"participant_id\": part, **agg})\n",
    "\n",
    "    if (i % VERBOSE_EVERY) == 0 or i == 1:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[audio] processed {i}/{len(audio_files)} ... ({dt:.1f}s)\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "audio_df = pd.DataFrame(rows)\n",
    "if audio_df.empty:\n",
    "    print(\"SKIP: no audio features aggregated.\")\n",
    "else:\n",
    "    audio_agg = audio_df.groupby(\"participant_id\").mean(numeric_only=True).reset_index()\n",
    "    audio_agg.to_parquet(AUDIO_OUT, index=False)\n",
    "    print(\"Saved audio features ->\", AUDIO_OUT, \"| shape=\", audio_agg.shape, \"| total time:\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ•·ï¸ Spider Check - Audio Features Peek\n",
    "\n",
    "Listening between the lines: prosody features like pitch, shimmer, and formants.  \n",
    "A quick peace-of-mind peek at participant audio stats to be sure the signals look right.  \n",
    "\n",
    "***Because voices carry more than words - they carry what's unseen.***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "AUDIO_OUT = PROCESSED_DIR / \"audio_features.parquet\"\n",
    "audio_agg = pd.read_parquet(AUDIO_OUT)\n",
    "\n",
    "print(\"Audio agg shape:\", audio_agg.shape)\n",
    "print(\"Nulls (top 10):\")\n",
    "print(audio_agg.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# ensure participant_id is str (helps merges)\n",
    "audio_agg[\"participant_id\"] = audio_agg[\"participant_id\"].astype(str)\n",
    "\n",
    "# quick peek\n",
    "display(audio_agg.head(3).iloc[:, :12])  # first dozen cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ•·ï¸ Spider Check - Multimodal Merge Peek\n",
    "\n",
    "Weaving the web: audio, text, and tabular features come together here.  \n",
    "A quick integrity check that strands align by participant_id before modeling.  \n",
    "\n",
    "***Because the strength of the web depends on what's unseen between the strands.***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEEK = True  # set False for speed; True for a quick spider check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Inventory - fast summary with optional peek()\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "proc_dir = ROOT_DIR / \"data\" / \"processed\"\n",
    "\n",
    "FILES = [\n",
    "    \"audio_features.parquet\",\n",
    "    \"text_meta.parquet\",\n",
    "    \"text_tfidf.parquet\",\n",
    "    \"text_tfidf_custom.parquet\",\n",
    "    \"text_idf_terms.csv\",\n",
    "    \"text_idf_terms_custom.csv\",\n",
    "    \"transcripts_unified.csv\",\n",
    "    \"tabular_phq8.parquet\",\n",
    "    \"multimodal_features.parquet\",\n",
    "]\n",
    "\n",
    "# ---- knobs ----\n",
    "PEEK = False          # flip to True when you want head(2)\n",
    "MAX_PEEK_COLS = 8     # cap preview width for mega-wide frames\n",
    "ENGINE = \"pyarrow\"    # or \"fastparquet\" if you prefer\n",
    "\n",
    "def _read(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix == \".parquet\":\n",
    "        return pd.read_parquet(path, engine=ENGINE)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def describe(path: Path, peek: bool = False) -> dict:\n",
    "    if not path.exists():\n",
    "        return {\"exists\": False}\n",
    "    try:\n",
    "        df = _read(path)\n",
    "        info = {\n",
    "            \"exists\": True,\n",
    "            \"shape\": df.shape,\n",
    "            \"columns\": df.columns[:5].tolist(),\n",
    "            \"non_nulls_top\": df.notna().sum().sort_values(ascending=False).head(5).to_dict(),\n",
    "            \"file_size_mb\": round(os.path.getsize(path) / (1024**2), 2),\n",
    "        }\n",
    "        if peek:\n",
    "            cols = df.columns[:MAX_PEEK_COLS]\n",
    "            info[\"peek\"] = df.loc[:, cols].head(2).to_dict(orient=\"records\")\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return {\"exists\": True, \"error\": str(e)}\n",
    "\n",
    "inv = {name: describe(proc_dir / name, peek=PEEK) for name in FILES}\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "pd.DataFrame(inv).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.4 Video (facial action units)\n",
    "- Use OpenFace to extract per frame AUs and gaze; aggregate per session. \n",
    "- Typical aggregates: mean, std, max, fraction above threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.4 Video (AUs/gaze) - per-participant aggregates (SKIP-safe)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Looks for files like \".../123_OpenFace.csv\" or \".../123_AUs.csv\"\n",
    "# Aggregates AU* and gaze_* numeric columns per participant (mean, std, median, p10/p90, #frames)\n",
    "# Saves to data/processed/video_features.parquet\n",
    "# =============================================================================\n",
    "import re, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "VIDEO_OUT = PROCESSED_DIR / \"video_features.parquet\"\n",
    "RAW_DIR   = ROOT_DIR / \"data\"\n",
    "\n",
    "def _pid_from_stem(stem: str) -> str:\n",
    "    m = re.match(r\"^(\\d+)\", stem)\n",
    "    return m.group(1) if m else stem\n",
    "\n",
    "video_files = list(RAW_DIR.rglob(\"*OpenFace*.csv\")) + list(RAW_DIR.rglob(\"*_AUs.csv\"))\n",
    "print(f\"Found {len(video_files)} video feature file(s)\")\n",
    "\n",
    "rows = []\n",
    "for p in video_files:\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(p, encoding=\"latin-1\")\n",
    "        except Exception:\n",
    "            print(\"SKIP: unreadable video file ->\", p.name); continue\n",
    "\n",
    "    # keep columns typically exported by OpenFace: AU*, gaze_*, head pose etc.\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    # prefer AU/gaze subset if present\n",
    "    keep_cols = [c for c in num.columns if c.startswith((\"AU\", \"gaze\", \"pose\", \"confidence\"))]\n",
    "    num = num[keep_cols] if keep_cols else num\n",
    "    if num.empty:\n",
    "        print(\"SKIP: no AU/gaze numeric columns in\", p.name); continue\n",
    "\n",
    "    part = _pid_from_stem(p.stem)\n",
    "    agg = {\n",
    "        **num.mean(numeric_only=True).add_prefix(\"mean_\").to_dict(),\n",
    "        **num.std(numeric_only=True).add_prefix(\"std_\").to_dict(),\n",
    "        **num.median(numeric_only=True).add_prefix(\"med_\").to_dict(),\n",
    "        **num.quantile(0.10, numeric_only=True).add_prefix(\"p10_\").to_dict(),\n",
    "        **num.quantile(0.90, numeric_only=True).add_prefix(\"p90_\").to_dict(),\n",
    "        \"n_frames\": len(num),\n",
    "    }\n",
    "    rows.append({\"participant_id\": part, **agg})\n",
    "\n",
    "video_df = pd.DataFrame(rows)\n",
    "if video_df.empty:\n",
    "    print(\"SKIP: no video features aggregated.\")\n",
    "else:\n",
    "    video_agg = video_df.groupby(\"participant_id\").mean(numeric_only=True).reset_index()\n",
    "    video_agg.to_parquet(VIDEO_OUT, index=False)\n",
    "    print(\"Saved video features ->\", VIDEO_OUT, \"| shape=\", video_agg.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Multimodal dataset assembly\n",
    "\n",
    "Merge per-modality feature tables on `['subject_id','session_id']`, align with labels, handle missing data, and validate splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4) Multimodal dataset assembly (tolerant to missing modalities)\n",
    "# Goal:\n",
    "#   - Merge per-modality feature tables by JOIN_KEY.\n",
    "#   - Handle absent files gracefully (print+SKIP).\n",
    "#   - Produce one row per participant with clear column prefixes.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MM_OUT = PROCESSED_DIR / \"multimodal_features.parquet\"\n",
    "\n",
    "def _safe_read_parquet(path: Path, note: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a parquet table if present; otherwise return an empty DF with JOIN_KEY.\"\"\"\n",
    "    if path.exists():\n",
    "        try:\n",
    "            df = pd.read_parquet(path)\n",
    "            print(f\"Loaded {note} -> {path} | shape={df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"SKIP read {note}: {type(e).__name__} - {e}\")\n",
    "    else:\n",
    "        print(f\"SKIP: {note} not found at {path}\")\n",
    "    return pd.DataFrame({JOIN_KEY: pd.Series(dtype=\"object\")})\n",
    "\n",
    "def _prefix(df: pd.DataFrame, prefix: str, skip_cols=(JOIN_KEY,)):\n",
    "    \"\"\"Prefix all non-ID columns to keep namespaces tidy.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    cols = {c: f\"{prefix}{c}\" for c in df.columns if c not in skip_cols}\n",
    "    return df.rename(columns=cols)\n",
    "\n",
    "# ---- Core labels (ID + target only) -----------------------------------------\n",
    "core = labels_df[[JOIN_KEY, TARGET]].drop_duplicates(subset=[JOIN_KEY]).copy()\n",
    "\n",
    "# ---- Tabular PHQ-8 ----------------------------------------------------------\n",
    "tab_p = _safe_read_parquet(PROCESSED_DIR / \"tabular_phq8.parquet\", \"PHQ-8 tabular\")\n",
    "# Drop duplicate target from tabular if present\n",
    "tab_p = tab_p.drop(columns=[c for c in [TARGET] if c in tab_p.columns], errors=\"ignore\")\n",
    "\n",
    "# ---- Text meta + base TF-IDF ------------------------------------------------\n",
    "tx_meta  = _safe_read_parquet(PROCESSED_DIR / \"text_meta.parquet\",  \"text meta\")\n",
    "tx_tfidf = _safe_read_parquet(PROCESSED_DIR / \"text_tfidf.parquet\", \"text TF-IDF\")\n",
    "\n",
    "# ---- (NEW) Custom-stoplist TF-IDF -------------------------------------------\n",
    "tx_tfidf_custom = _safe_read_parquet(PROCESSED_DIR / \"text_tfidf_custom.parquet\", \"text TF-IDF (custom)\")\n",
    "\n",
    "# ---- Prefix columns to avoid collisions -------------------------------------\n",
    "tab_p           = _prefix(tab_p,           \"tab_\")\n",
    "tx_meta         = _prefix(tx_meta,         \"txt_\")\n",
    "tx_tfidf        = _prefix(tx_tfidf,        \"tfidf_\")\n",
    "tx_tfidf_custom = _prefix(tx_tfidf_custom, \"tfidfC_\")\n",
    "\n",
    "# ---- Merge everything on JOIN_KEY -------------------------------------------\n",
    "mm = core.merge(tab_p,           on=JOIN_KEY, how=\"left\")\n",
    "mm = mm.merge(tx_meta,           on=JOIN_KEY, how=\"left\")\n",
    "mm = mm.merge(tx_tfidf,          on=JOIN_KEY, how=\"left\")\n",
    "mm = mm.merge(tx_tfidf_custom,   on=JOIN_KEY, how=\"left\")  # add custom TF-IDF\n",
    "\n",
    "# ---- Final sanity + save -----------------------------------------------------\n",
    "print(\"Multimodal shape:\", mm.shape)\n",
    "mm.to_parquet(MM_OUT, index=False)\n",
    "print(\"Saved multimodal ->\", MM_OUT)\n",
    "\n",
    "# Small preview: ID, target, a few PHQ-8 features, and text QC columns\n",
    "peek_cols = [JOIN_KEY, TARGET]\n",
    "peek_cols += [c for c in mm.columns if c.startswith(\"tab_phq8_\")][:5]\n",
    "peek_cols += [c for c in mm.columns if c.startswith(\"txt_text_len_\")][:2]\n",
    "print(\"Preview:\")\n",
    "print(mm[[c for c in peek_cols if c in mm.columns]].head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Artifacts (saved processed data)\n",
    "Save per modality tables and the merged multimodal dataset for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ART_TEXT = PROC_DIR / 'text_embeddings.parquet'\n",
    "ART_AUDIO = PROC_DIR / 'audio_features.parquet'\n",
    "ART_VIDEO = PROC_DIR / 'video_features.parquet'\n",
    "ART_TAB = PROC_DIR / 'phq8_engineered.parquet'\n",
    "ART_MERGE = PROC_DIR / 'multimodal_merged.parquet'\n",
    "\n",
    "# Save only if non-empty (avoid writing empty placeholder frames)\n",
    "if not df_tab.empty: df_tab.to_parquet(ART_TAB, index=False)\n",
    "if not df_text_model.empty: df_text_model.to_parquet(ART_TEXT, index=False)\n",
    "if not df_audio.empty: df_audio.to_parquet(ART_AUDIO, index=False)\n",
    "if not df_video.empty: df_video.to_parquet(ART_VIDEO, index=False)\n",
    "if not df_features.empty: df_features.to_parquet(ART_MERGE, index=False)\n",
    "\n",
    "print('Saved (if available):')\n",
    "for p in [ART_TAB, ART_TEXT, ART_AUDIO, ART_VIDEO, ART_MERGE]:\n",
    " print('-', p, p.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Limitations & reproducibility\n",
    "\n",
    "**Limitations**\n",
    "- Placeholder tables for text/audio/video until extraction pipelines are connected. \n",
    "- Class imbalance persists; monitor PR curves and calibration in later notebooks. \n",
    "- Alignment between modalities may vary by session; verify IDs and time boundaries upstream.\n",
    "\n",
    "**Reproducibility**\n",
    "- Python 3.11 (`venv`) \n",
    "- Core libs: `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `z3-solver` \n",
    "- Optional libs: `sentence-transformers`, `librosa`/`opensmile`, `openface` (CLI) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Closing summary & next steps\n",
    "\n",
    "- Engineered tabular features (PHQ 8 with standardized interactions). \n",
    "- Added SMT guardrails to catch data/timing/split issues early. \n",
    "- Assembled a merged, subject-disjoint multimodal dataset ready for modeling.\n",
    "\n",
    "**Next:** Notebook 04 - Multimodal Modeling & Fusion (LR/RF/baseline NN; late fusion vs. early fusion; calibration & interpretability).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trauma_ai)",
   "language": "python",
   "name": "trauma_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
