{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook 03 - Feature Engineering & Multimodal Inputs\n",
    "\n",
    "**Project:** Recognizing the Unseen - A Multimodal, Trauma-Informed AI Framework \n",
    "**Goal of this notebook:** engineer features beyond PHQ-8 and prepare multimodal inputs (text, audio, video) for downstream modeling.\n",
    "\n",
    "**Builds on:** \n",
    "- Notebook 01: Import, clean, EDA (labels + minimal cleaning) \n",
    "- Notebook 02: Baselines (Dummy vs. Logistic), ROC/PR, coefficient plots, interactive sliders + thresholds \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "## Contents\n",
    "1. Data sources & setup \n",
    "2. SMT guardrails (Z3) for data integrity and split hygiene \n",
    "3. Feature engineering \n",
    " - Tabular (PHQ-8) \n",
    " - Text (transcripts embeddings) \n",
    " - Audio (prosody) \n",
    " - Video (facial action units) \n",
    "4. Multimodal dataset assembly \n",
    "5. Artifacts (saved processed data) \n",
    "6. Limitations & reproducibility \n",
    "7. Closing summary & next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data sources & setup\n",
    "\n",
    "Load the cleaned PHQ-8 labels and set up placeholders for additional modalities. \n",
    "This cell focuses on reading already-prepared artifacts from prior notebooks and defining\n",
    "conventions for participant/session keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports & Paths  +  Data sources & setup (canonical one-stop block)\n",
    "# -----------------------------------------------------------------------------\n",
    "# - Imports (numpy/pandas, etc.) + quick diagnostics (Python/pandas version)\n",
    "# - Resolve repo root and standard data directories\n",
    "# - Define canonical join/target names used across Notebook 03\n",
    "# - Load cleaned labels (from Notebook 01/02 output) with graceful fallback\n",
    "# - Normalize common column-name variants -> {participant_id, label, split}\n",
    "# - Print a concise summary for reviewers\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports -----------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# (Optional) quick diagnostics to make runtime context explicit\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "# Optional: show whether any BLAS info is available (won't crash if missing)\n",
    "blas = getattr(getattr(np, \"__config__\", object()), \"blas_opt_info\", {})\n",
    "print(\"BLAS info found:\", bool(blas))\n",
    "\n",
    "# --- Paths (single source of truth) ------------------------------------------\n",
    "# Reuse ROOT_DIR if already set (e.g., by earlier cells); otherwise derive it.\n",
    "try:\n",
    "    ROOT_DIR\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().resolve().parent  # notebooks/ -> repo root\n",
    "\n",
    "DATA_DIR      = ROOT_DIR / \"data\"\n",
    "CLEAN_DIR     = DATA_DIR / \"cleaned\"       # outputs from NB01/NB02 (adjust if your layout differs)\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"     # downstream features go here\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Canonical names & expected labels artifact ------------------------------\n",
    "JOIN_KEY = \"participant_id\"                # canonical join key used throughout NB03\n",
    "TARGET   = \"label\"                         # canonical binary target (0/1)\n",
    "LABELS_PATH = CLEAN_DIR / \"labels_clean.parquet\"   # <- adjust here if NB01/02 writes elsewhere\n",
    "\n",
    "# --- Load labels with guardrails (degrade gracefully if file not ready) ------\n",
    "if LABELS_PATH.exists():\n",
    "    labels_df = pd.read_parquet(LABELS_PATH)\n",
    "    print(f\"Loaded labels: {LABELS_PATH} | shape={labels_df.shape}\")\n",
    "else:\n",
    "    print(f'NOTE: labels_clean.parquet not found -> {LABELS_PATH}')\n",
    "    print(\"      (Run Notebook 01/02 to generate, or adjust LABELS_PATH if it moved.)\")\n",
    "    # Create empty skeleton so downstream checks in Step 2 can SKIP cleanly\n",
    "    labels_df = pd.DataFrame(columns=[JOIN_KEY, TARGET, \"split\"])\n",
    "\n",
    "# --- Normalize column names to our canonical schema --------------------------\n",
    "rename_map = {}\n",
    "# join key variants\n",
    "if \"subject_id\" in labels_df.columns and JOIN_KEY not in labels_df.columns:\n",
    "    rename_map[\"subject_id\"] = JOIN_KEY\n",
    "if \"id\" in labels_df.columns and JOIN_KEY not in labels_df.columns:\n",
    "    rename_map[\"id\"] = JOIN_KEY\n",
    "# target variants\n",
    "if \"target\" in labels_df.columns and TARGET not in labels_df.columns:\n",
    "    rename_map[\"target\"] = TARGET\n",
    "if \"phq8_binary\" in labels_df.columns and TARGET not in labels_df.columns:\n",
    "    rename_map[\"phq8_binary\"] = TARGET\n",
    "\n",
    "if rename_map:\n",
    "    labels_df = labels_df.rename(columns=rename_map)\n",
    "\n",
    "# Ensure placeholders exist (keeps downstream guardrails readable & safe)\n",
    "if JOIN_KEY not in labels_df.columns:\n",
    "    labels_df[JOIN_KEY] = pd.Series(dtype=\"object\")\n",
    "    print(f'NOTE: added empty \"{JOIN_KEY}\" column')\n",
    "if TARGET not in labels_df.columns:\n",
    "    labels_df[TARGET] = pd.Series(dtype=\"Int64\")  # nullable int; matches {0,1}\n",
    "    print(f'NOTE: added empty \"{TARGET}\" column')\n",
    "if \"split\" not in labels_df.columns:\n",
    "    labels_df[\"split\"] = pd.Series(dtype=\"string\")\n",
    "    print('NOTE: added empty \"split\" column')\n",
    "\n",
    "# --- Reviewer-friendly summary ----------------------------------------------\n",
    "n_rows = len(labels_df)\n",
    "sample_cols = [c for c in [JOIN_KEY, TARGET, \"split\"] if c in labels_df.columns]\n",
    "print(f\"labels_df: {n_rows} rows, {labels_df.shape[1]} cols | has {sample_cols} | head:\")\n",
    "print(labels_df[sample_cols].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "\n",
    "## 2) SMT guardrails (Z3) for data integrity and split hygiene\n",
    "\n",
    "We add lightweight **formal checks** to catch structural mistakes early:\n",
    "\n",
    "- Temporal event sanity: `onset < apex < offset n_frames - 1` \n",
    "- Window safety: each feature window stays within clip bounds \n",
    "- Sampling consistency: `fps > 0` and `duration frames / fps` \n",
    "- Split hygiene: subject-disjoint train/val/test; minimum class presence per split \n",
    "- Label domain checks: labels belong to the expected set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2 â€” SMT GUARDRAILS (Z3) + SPLIT HYGIENE\n",
    "# Goal:\n",
    "#   - Enforce label domain and (optionally) split integrity with small, readable\n",
    "#     checks that fail-fast when assumptions break.\n",
    "#   - Keep notebook executable even when artifacts are not ready (print & skip).\n",
    "# Why:\n",
    "#   - Early structural checks catch silent drift (e.g., wrong label domain, ID\n",
    "#     overlap across splits) before modeling.\n",
    "# =============================================================================\n",
    "\n",
    "# Make repo-root imports work from inside notebooks/\n",
    "# Why: the kernel's CWD is often `notebooks/`, while `verification.py` lives at repo root.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_repo_root(filename: str = \"verification.py\") -> Path | None:\n",
    "    \"\"\"Walk upward from CWD until `filename` is found; return its parent (repo root).\"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / filename).exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Reuse ROOT_DIR from Step 1 if present; otherwise resolve it robustly here.\n",
    "try:\n",
    "    ROOT_DIR\n",
    "except NameError:\n",
    "    ROOT_DIR = _find_repo_root() or Path.cwd().resolve().parent  # fallback: notebooks/ -> repo root\n",
    "\n",
    "# Ensure the root is importable\n",
    "if ROOT_DIR and str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Now safe(ish) to import guardrail utilities; if unavailable, degrade gracefully.\n",
    "_guardrails_loaded = False\n",
    "try:\n",
    "    from verification import (\n",
    "        check_event_triplet,             # example timing check for onset/apex/offset\n",
    "        check_window_bounds,             # window [start, start+len) within [0, n)\n",
    "        check_sampling_consistency,      # duration â‰ˆ frames/fps\n",
    "        assert_disjoint_splits,          # no subject overlap across splits\n",
    "        min_class_presence,              # per-split class counts â‰¥ threshold\n",
    "        assert_label_domain,             # labels in allowed set\n",
    "        verify_env,                      # tiny runtime report for smoke tests/CI\n",
    "    )\n",
    "    _guardrails_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"SKIP: guardrail utilities not importable ({type(e).__name__}: {e}). \"\n",
    "          \"Proceeding without hard checks so the notebook stays runnable.\")\n",
    "\n",
    "JOIN_KEY = \"participant_id\"\n",
    "TARGET   = \"label\"\n",
    "\n",
    "# ---- 2.1 Label hygiene ------------------------------------------------------\n",
    "if not _guardrails_loaded:\n",
    "    print(\"SKIP: label checks (verification.py not loaded).\")\n",
    "elif \"labels_df\" not in globals() or labels_df is None or labels_df.empty or TARGET not in labels_df.columns:\n",
    "    print(\"SKIP: label checks (labels_df empty or target column missing).\")\n",
    "else:\n",
    "    # Domain guarantee â†’ reviewers see intent: binary classification (0/1).\n",
    "    assert_label_domain(labels_df[TARGET], allowed=(0, 1))\n",
    "    print(\"OK: label domain is restricted to {0, 1}.\")\n",
    "\n",
    "# ---- 2.2 Split hygiene (optional) -------------------------------------------\n",
    "# If you already created a split in Notebook 02, this validates it.\n",
    "if not _guardrails_loaded:\n",
    "    print(\"SKIP: split checks (verification.py not loaded). \"\n",
    "          \"Create deterministic splits in Notebook 02/03 before modeling.\")\n",
    "elif (\"labels_df\" in globals() and labels_df is not None and not labels_df.empty\n",
    "      and (\"split\" in labels_df.columns) and (JOIN_KEY in labels_df.columns)):\n",
    "    # Extract subject IDs per split (keeps checks explainable & auditable).\n",
    "    train_ids = labels_df.loc[labels_df[\"split\"] == \"train\", JOIN_KEY]\n",
    "    val_ids   = labels_df.loc[labels_df[\"split\"] == \"val\",   JOIN_KEY]\n",
    "    test_ids  = labels_df.loc[labels_df[\"split\"] == \"test\",  JOIN_KEY]\n",
    "\n",
    "    # (a) No subject overlap across splits\n",
    "    assert_disjoint_splits(train_ids, val_ids, test_ids)\n",
    "    print(\"OK: no subject overlap across splits (train/val/test).\")\n",
    "\n",
    "    # (b) Minimum per-class support in each split â†’ guards against degenerate folds\n",
    "    min_class_presence(\n",
    "        {\n",
    "            \"train\": labels_df.loc[labels_df[\"split\"] == \"train\", TARGET],\n",
    "            \"val\":   labels_df.loc[labels_df[\"split\"] == \"val\",   TARGET],\n",
    "            \"test\":  labels_df.loc[labels_df[\"split\"] == \"test\",  TARGET],\n",
    "        },\n",
    "        min_count=5  # Adjust with dataset size; aim to preserve evaluation stability.\n",
    "    )\n",
    "    print(\"OK: each split meets minimum class presence thresholds.\")\n",
    "else:\n",
    "    print('SKIP: split checks (no \"split\" column yet). '\n",
    "          \"Create deterministic splits in Notebook 02/03 before modeling.\")\n",
    "\n",
    "# ---- 2.3 Timing/window sanity (optional, runs only if variables provided) ---\n",
    "# These are examples; they will quietly skip if you haven't defined the inputs yet.\n",
    "# Rationale: keeps nbconvert/CI green while still documenting expectations.\n",
    "\n",
    "if _guardrails_loaded:\n",
    "    # Example A: sampling consistency for a video segment: frames / fps â‰ˆ duration\n",
    "    try:\n",
    "        ok, msg = check_sampling_consistency(\n",
    "            frames=int(video_frames),        # define upstream when available\n",
    "            fps=float(video_fps),\n",
    "            duration_sec=float(video_duration_sec)\n",
    "        )\n",
    "        print(\"Video sampling check:\", msg)\n",
    "    except Exception:\n",
    "        # Not available yet; that is expected in early drafts.\n",
    "        pass\n",
    "\n",
    "    # Example B: generic window bounds (e.g., feature extraction slices)\n",
    "    try:\n",
    "        ok, msg = check_window_bounds(\n",
    "            start=int(win_start),            # define upstream when available\n",
    "            length=int(win_len),\n",
    "            n_frames=int(total_frames)\n",
    "        )\n",
    "        print(\"Window bounds check:\", msg)\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"SKIP: timing/window checks (verification.py not loaded).\")\n",
    "\n",
    "print(\"Guardrail checks completed.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Smoke test â€” confirm guardrail utilities are importable and show env facts\n",
    "# =============================================================================\n",
    "_loaded = globals().get(\"_guardrails_loaded\", False)\n",
    "\n",
    "if _loaded:\n",
    "    try:\n",
    "        import verification\n",
    "        print(f\"Verification module loaded from: {verification.__file__}\")\n",
    "        want = [\n",
    "            \"check_event_triplet\",\n",
    "            \"check_window_bounds\",\n",
    "            \"check_sampling_consistency\",\n",
    "            \"assert_disjoint_splits\",\n",
    "            \"min_class_presence\",\n",
    "            \"assert_label_domain\",\n",
    "            \"verify_env\",\n",
    "        ]\n",
    "        available = [name for name in want if getattr(verification, name, None)]\n",
    "        missing   = [name for name in want if name not in available]\n",
    "        print(\"Available guardrail functions:\", available)\n",
    "        if missing:\n",
    "            print(\"Note: missing in verification.py ->\", missing)\n",
    "        # One-line environment report (nice for CI and Dr. S)\n",
    "        try:\n",
    "            print(\"Env:\", verification.verify_env())\n",
    "        except Exception:\n",
    "            print(\"Env: verify_env() raised; skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Smoke test warning: import succeeded but inspection failed ({type(e).__name__}: {e})\")\n",
    "else:\n",
    "    print(\"Smoke test: verification.py not loaded (see SKIP messages above).\")\n",
    "\n",
    "# Show where ROOT_DIR resolved to (useful for CI/review logs)\n",
    "print(\"Resolved ROOT_DIR:\", ROOT_DIR if \"ROOT_DIR\" in globals() else \"<not set>\")\n",
    "\n",
    "# Peek at the first few sys.path entries to confirm import order\n",
    "print(\"sys.path[0:3]:\", sys.path[:3])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "> ðŸ’¡ **Workflow tip:** Run the checks immediately after loading each modality. Fail fast with clear errors so issues don't propagate into modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Feature engineering\n",
    "We create modality-specific features. Start simple and keep everything **reproducible**.\n",
    "\n",
    "### 3.1 Tabular (PHQ-8)\n",
    "- Standardize numeric PHQ-8 items.\n",
    "- (Optional) Create low-order interaction terms for hypothesis-driven pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick probe: see all columns that look PHQ-related\n",
    "[c for c in labels_df.columns if \"phq\" in str(c).lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 Tabular (PHQ-8) â€” Clinical-style imputation + optional rounding\n",
    "# Goal:\n",
    "#   - Build interpretable PHQ-8 features (sum/mean/missingness, z-scores).\n",
    "#   - Clinical scoring:\n",
    "#       * If â‰¤1 item missing â†’ impute that item with the row mean, then sum.\n",
    "#       * If â‰¥2 items missing â†’ leave score NaN (no aggressive imputation).\n",
    "#   - Optional rounding of the final score to match reporting conventions.\n",
    "#   - After scoring, zero-fill item columns for downstream models (documented).\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "TAB_OUT = PROCESSED_DIR / \"tabular_phq8.parquet\"\n",
    "\n",
    "# ---- Explicit PHQ-8 schema pin (order matters: items 1..8) ------------------\n",
    "PHQ8_COLS = [\n",
    "    \"phq8_nointerest\",      # 1  little interest/pleasure\n",
    "    \"phq8_depressed\",       # 2  feeling down/depressed/hopeless\n",
    "    \"phq8_sleep\",           # 3  sleep problems\n",
    "    \"phq8_tired\",           # 4  low energy/tired\n",
    "    \"phq8_appetite\",        # 5  appetite/eating\n",
    "    \"phq8_failure\",         # 6  feeling bad/failure/worthless/guilty\n",
    "    \"phq8_concentrating\",   # 7  trouble concentrating\n",
    "    \"phq8_moving\",          # 8  psychomotor (restless/slow)\n",
    "]\n",
    "REQUIRE_ALL_ITEMS = True  # set False to proceed if any items missing\n",
    "\n",
    "# Choose rounding for the total score: \"nearest\" | \"bankers\" | \"floor\" | \"ceil\" | None\n",
    "SCORE_ROUNDING = \"nearest\"\n",
    "\n",
    "# ---- Guard schema presence ---------------------------------------------------\n",
    "missing_items = [c for c in PHQ8_COLS if c not in labels_df.columns]\n",
    "if missing_items:\n",
    "    msg = f\"PHQ-8 schema mismatch: missing {len(missing_items)} column(s): {missing_items}\"\n",
    "    if REQUIRE_ALL_ITEMS:\n",
    "        raise AssertionError(msg)\n",
    "    else:\n",
    "        print(\"WARNING:\", msg, \"â†’ proceeding with available items only.\")\n",
    "        PHQ8_COLS = [c for c in PHQ8_COLS if c in labels_df.columns]\n",
    "\n",
    "if not PHQ8_COLS:\n",
    "    print(\"SKIP: No PHQ-8 item columns available; tabular features will be empty.\")\n",
    "    tab_df = pd.DataFrame(columns=[JOIN_KEY, TARGET])\n",
    "else:\n",
    "    # ---- 3.1.1 Assemble base frame ------------------------------------------\n",
    "    base_cols = [c for c in [JOIN_KEY, TARGET] if c in labels_df.columns]\n",
    "    tab_df = labels_df[base_cols + PHQ8_COLS].copy()\n",
    "\n",
    "    # Coerce items to numeric safely (handles stray strings gracefully)\n",
    "    items = tab_df[PHQ8_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # ---- 3.1.2 Clinical-style imputation & scoring ---------------------------\n",
    "    missing_ct = items.isna().sum(axis=1)        # items missing per row\n",
    "    row_mean   = items.mean(axis=1, skipna=True) # mean of answered items\n",
    "\n",
    "    # Impute only when exactly 1 (or â‰¤1) item missing\n",
    "    items_imputed = items.copy()\n",
    "    mask_impute = missing_ct.le(1) & missing_ct.gt(0)  # (0 < missing â‰¤ 1)\n",
    "    items_imputed.loc[mask_impute] = (\n",
    "        items_imputed.loc[mask_impute].T\n",
    "        .fillna(row_mean[mask_impute])  # broadcast row-wise means into NaNs\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    # Score:\n",
    "    #  - If â‰¥2 items missing â†’ keep NaN (min_count enforces that)\n",
    "    #  - Else â†’ sum imputed row\n",
    "    tab_df[\"phq8_missing_count\"] = missing_ct\n",
    "    tab_df[\"phq8_sum\"]  = items_imputed.sum(axis=1, min_count=len(PHQ8_COLS) - 1)\n",
    "    tab_df[\"phq8_mean\"] = items_imputed.mean(axis=1, skipna=True)\n",
    "\n",
    "    # ---- 3.1.3 Optional rounding to match reporting conventions -------------\n",
    "    if SCORE_ROUNDING == \"nearest\":\n",
    "        s = tab_df[\"phq8_sum\"]\n",
    "        tab_df[\"phq8_sum\"] = np.sign(s) * np.floor(np.abs(s) + 0.5)  # half-away-from-zero\n",
    "    elif SCORE_ROUNDING == \"bankers\":\n",
    "        tab_df[\"phq8_sum\"] = tab_df[\"phq8_sum\"].round(0)\n",
    "    elif SCORE_ROUNDING == \"floor\":\n",
    "        tab_df[\"phq8_sum\"] = np.floor(tab_df[\"phq8_sum\"])\n",
    "    elif SCORE_ROUNDING == \"ceil\":\n",
    "        tab_df[\"phq8_sum\"] = np.ceil(tab_df[\"phq8_sum\"])\n",
    "    # else: leave fractional totals as-is\n",
    "\n",
    "    # ---- 3.1.4 Post-scoring zero-fill for model inputs (documented choice) ---\n",
    "    # Keeps rows dense for models while preserving clinically faithful 'phq8_sum'.\n",
    "    tab_df[PHQ8_COLS] = items.fillna(0)\n",
    "\n",
    "    # ---- 3.1.5 Standardize numeric features (excluding target & ID) ----------\n",
    "    num_cols = [c for c in tab_df.columns\n",
    "                if c not in [JOIN_KEY, TARGET] and pd.api.types.is_numeric_dtype(tab_df[c])]\n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        tab_df[[f\"{c}_z\" for c in num_cols]] = scaler.fit_transform(tab_df[num_cols])\n",
    "        print(f\"Scaled {len(num_cols)} numeric columns -> *_z\")\n",
    "    else:\n",
    "        print(\"NOTE: No numeric columns to scale.\")\n",
    "\n",
    "# ---- 3.1.6 Save & reviewer preview -----------------------------------------\n",
    "try:\n",
    "    tab_df.to_parquet(TAB_OUT, index=False)\n",
    "    print(\"Saved tabular PHQ-8 ->\", TAB_OUT, \"| shape=\", tab_df.shape)\n",
    "except Exception as e:\n",
    "    print(\"SKIP save:\", type(e).__name__, \"-\", e)\n",
    "\n",
    "show_cols = [JOIN_KEY, TARGET] + PHQ8_COLS + [\"phq8_missing_count\", \"phq8_sum\", \"phq8_mean\"]\n",
    "show_cols = [c for c in show_cols if c in tab_df.columns]\n",
    "print(\"tab_df preview:\")\n",
    "print(tab_df[show_cols].head(5))\n",
    "\n",
    "# ---- Optional QA against any provided 'phq8_score' column -------------------\n",
    "if \"phq8_score\" in labels_df.columns:\n",
    "    try:\n",
    "        orig = pd.to_numeric(labels_df[\"phq8_score\"], errors=\"coerce\")\n",
    "        agree = (orig.fillna(-1).astype(float) == tab_df[\"phq8_sum\"].fillna(-2).astype(float)).sum()\n",
    "        print(f\"QA: phq8_sum (clinical + rounding) vs phq8_score agreement: {agree}/{len(tab_df)} rows\")\n",
    "    except Exception:\n",
    "        print(\"QA: could not compare to 'phq8_score' (non-fatal).\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHQ-8 QA (optional): compare our phq8_sum to provided phq8_score\n",
    "# =============================================================================\n",
    "if \"phq8_score\" in labels_df.columns and \"phq8_sum\" in tab_df.columns:\n",
    "    orig = pd.to_numeric(labels_df[\"phq8_score\"], errors=\"coerce\")\n",
    "    ours = pd.to_numeric(tab_df[\"phq8_sum\"], errors=\"coerce\")\n",
    "\n",
    "    mismask = orig.fillna(-1).astype(float) != ours.fillna(-2).astype(float)\n",
    "    mism_idx = mismask[mismask].index\n",
    "    n_mis = int(mismask.sum())\n",
    "\n",
    "    print(f\"QA: mismatches (ours vs provided): {n_mis}/{len(tab_df)} rows\")\n",
    "    if n_mis:\n",
    "        cols = [JOIN_KEY, \"phq8_score\", \"phq8_sum\", \"phq8_mean\", \"phq8_missing_count\"] + PHQ8_COLS\n",
    "        # Show up to 5 examples\n",
    "        preview = tab_df.loc[mism_idx, [c for c in cols if c in tab_df.columns]].head(5).copy()\n",
    "        # Add the provided score for clarity (from labels_df)\n",
    "        preview[\"phq8_score_src\"] = labels_df.loc[preview.index, \"phq8_score\"]\n",
    "        display(preview)\n",
    "else:\n",
    "    print(\"QA: skipped (no 'phq8_score' column or 'phq8_sum' not computed).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "### PHQ-8 tabular features: interpretation & key takeaways\n",
    "\n",
    "**What we did**\n",
    "- Pinned PHQ-8 item schema: [\"phq8_nointerest\",\"phq8_depressed\",\"phq8_sleep\",\"phq8_tired\",\"phq8_appetite\",\"phq8_failure\",\"phq8_concentrating\",\"phq8_moving\"].\n",
    "- Clinical-style scoring:\n",
    "  - If â‰¤1 item missing: imputed the missing item with the row mean of answered items, then summed.\n",
    "  - If â‰¥2 items missing: left the score as NaN (no aggressive imputation).\n",
    "- Optional rounding: set to \"nearest\" so totals match typical reporting.\n",
    "- After scoring, zero-filled item columns for modeling, and z-scored numeric features for comparability.\n",
    "\n",
    "**Guardrails & QA**\n",
    "- Label domain and split checks run in Step 2 (fail-fast or SKIP cleanly).\n",
    "- PHQ-8 QA: our computed \"phq8_sum\" vs provided \"phq8_score\" â†’ **107/107** agreement with rounding (\"nearest\").\n",
    "\n",
    "**Results snapshot**\n",
    "- Saved to `data/processed/tabular_phq8.parquet`.\n",
    "- Shape: **(107, 24)** (ID, label, 8 items, missing_count, sum, mean, and z-scored variants).\n",
    "- Missingness: `phq8_missing_count` shows per-row item gaps; rows with â‰¥2 missing keep `phq8_sum` as NaN.\n",
    "\n",
    "**How to read the features**\n",
    "- `phq8_sum`: total symptom burden (higher = more severe).\n",
    "- `phq8_mean`: average per-item severity (robust when one item is imputed).\n",
    "- `phq8_missing_count`: data quality indicator; consider as a covariate or filter in sensitivity analyses.\n",
    "- `*_z`: standardized versions for models that benefit from scaled inputs.\n",
    "\n",
    "**Decisions (documented)**\n",
    "- Rounding: used \"nearest\" to mirror the provided clinical scores (prevents off-by-one drift when one item is imputed).\n",
    "- Post-scoring zero-fill: keeps downstream models dense without altering the clinically faithful `phq8_sum`.\n",
    "\n",
    "**Limitations**\n",
    "- Row-mean imputation for a single missing item is simple and standard, but still an assumption.\n",
    "- Rows with â‰¥2 missing items are not scored; downstream models should either ignore `phq8_sum` for those rows or handle NaNs explicitly.\n",
    "\n",
    "**Recommended next steps**\n",
    "- Sensitivity check: run models with and without rounding; confirm conclusions are stable.\n",
    "- Optionally add `phq8_flag_gt1_missing = 1{missing_count â‰¥ 2}` as an exclusion flag or covariate.\n",
    "- Proceed to 3.2 (Text) to add linguistic signals; the tabular block provides a solid baseline.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Text (transcripts embeddings)\n",
    "- Option A (quick baseline): TF IDF on transcript text. \n",
    "- Option B (semantic): sentence embeddings (e.g., SentenceTransformers).\n",
    "\n",
    "> Note: If running offline or with limited resources, prefer TF IDF first; swap in embeddings later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder stub for text features (replace with real transcript loading)\n",
    "# Example API (choose one approach):\n",
    "USE_TFIDF = True\n",
    "\n",
    "if not df_text.empty:\n",
    " if USE_TFIDF:\n",
    " from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " vec = TfidfVectorizer(max_features=1024, ngram_range=(1,2))\n",
    " tfidf = vec.fit_transform(df_text.get('transcript', pd.Series([], dtype=str)).fillna(''))\n",
    " df_text_feats = pd.DataFrame(tfidf.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf.shape[1])])\n",
    " else:\n",
    " # Sentence embeddings (requires sentence-transformers)\n",
    " # from sentence_transformers import SentenceTransformer\n",
    " # model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    " # emb = model.encode(df_text['transcript'].fillna('').tolist(), batch_size=64, show_progress_bar=True)\n",
    " # df_text_feats = pd.DataFrame(emb, columns=[f'emb_{i}' for i in range(emb.shape[1])])\n",
    " df_text_feats = pd.DataFrame() # placeholder\n",
    " df_text_model = pd.concat([df_text[['subject_id','session_id']].reset_index(drop=True),\n",
    " df_text_feats.reset_index(drop=True)], axis=1)\n",
    "else:\n",
    " df_text_model = df_text.copy()\n",
    " print('Text table empty; populate df_text with transcripts + IDs.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Audio (prosody)\n",
    "- Fundamental frequency (f0), jitter/shimmer, loudness/energy, spectral features. \n",
    "- Extract with OpenSMILE or COVAREP, then aggregate per session (mean, std, percentiles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder stub for audio features\n",
    "# Expect df_audio to already contain aggregated features keyed by subject/session.\n",
    "if df_audio.empty:\n",
    " print('Audio table empty; populate df_audio with prosodic aggregates (e.g., f0_mean, jitter, etc.).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Video (facial action units)\n",
    "- Use OpenFace to extract per frame AUs and gaze; aggregate per session. \n",
    "- Typical aggregates: mean, std, max, fraction above threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder stub for video features\n",
    "# Expect df_video to already contain aggregated AU/gaze features keyed by subject/session.\n",
    "if df_video.empty:\n",
    " print('Video table empty; populate df_video with AU/gaze aggregates.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Multimodal dataset assembly\n",
    "\n",
    "Merge per-modality feature tables on `['subject_id','session_id']`, align with labels, handle missing data, and validate splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge order: tabular -> text -> audio -> video\n",
    "dfs = []\n",
    "for d in [df_tab, df_text_model, df_audio, df_video]:\n",
    " if not d.empty:\n",
    " dfs.append(d)\n",
    "\n",
    "if dfs:\n",
    " from functools import reduce\n",
    " df_features = reduce(lambda left, right: pd.merge(left, right, on=['subject_id','session_id'], how='outer'), dfs)\n",
    "else:\n",
    " df_features = pd.DataFrame(columns=['subject_id','session_id'])\n",
    "\n",
    "# Attach target if available\n",
    "if 'depressed' in df_labels.columns:\n",
    " df_features = pd.merge(df_labels[['subject_id','session_id','depressed']], df_features, on=['subject_id','session_id'], how='left')\n",
    "\n",
    "print('Multimodal merged shape:', df_features.shape)\n",
    "\n",
    "# Simple imputation (zero-fill for missing engineered features; leave IDs/target intact)\n",
    "id_cols = ['subject_id','session_id','depressed']\n",
    "feat_cols = [c for c in df_features.columns if c not in id_cols]\n",
    "df_features[feat_cols] = df_features[feat_cols].fillna(0.0)\n",
    "\n",
    "# Split by subject (subject-disjoint)\n",
    "subjects = df_features['subject_id'].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(subjects)\n",
    "n = len(subjects)\n",
    "train_ids = subjects[: int(0.7*n)]\n",
    "val_ids = subjects[int(0.7*n): int(0.85*n)]\n",
    "test_ids = subjects[int(0.85*n):]\n",
    "\n",
    "train = df_features[df_features['subject_id'].isin(train_ids)]\n",
    "val = df_features[df_features['subject_id'].isin(val_ids)]\n",
    "test = df_features[df_features['subject_id'].isin(test_ids)]\n",
    "\n",
    "print('Split sizes (rows):', len(train), len(val), len(test))\n",
    "\n",
    "# Split hygiene checks\n",
    "assert_disjoint_splits(train['subject_id'], val['subject_id'], test['subject_id'])\n",
    "assert_label_domain(df_features['depressed'], allowed=(0,1))\n",
    "min_class_presence({'train': train['depressed'], 'val': val['depressed'], 'test': test['depressed']}, min_count=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Artifacts (saved processed data)\n",
    "Save per modality tables and the merged multimodal dataset for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ART_TEXT = PROC_DIR / 'text_embeddings.parquet'\n",
    "ART_AUDIO = PROC_DIR / 'audio_features.parquet'\n",
    "ART_VIDEO = PROC_DIR / 'video_features.parquet'\n",
    "ART_TAB = PROC_DIR / 'phq8_engineered.parquet'\n",
    "ART_MERGE = PROC_DIR / 'multimodal_merged.parquet'\n",
    "\n",
    "# Save only if non-empty (avoid writing empty placeholder frames)\n",
    "if not df_tab.empty: df_tab.to_parquet(ART_TAB, index=False)\n",
    "if not df_text_model.empty: df_text_model.to_parquet(ART_TEXT, index=False)\n",
    "if not df_audio.empty: df_audio.to_parquet(ART_AUDIO, index=False)\n",
    "if not df_video.empty: df_video.to_parquet(ART_VIDEO, index=False)\n",
    "if not df_features.empty: df_features.to_parquet(ART_MERGE, index=False)\n",
    "\n",
    "print('Saved (if available):')\n",
    "for p in [ART_TAB, ART_TEXT, ART_AUDIO, ART_VIDEO, ART_MERGE]:\n",
    " print('-', p, p.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Limitations & reproducibility\n",
    "\n",
    "**Limitations**\n",
    "- Placeholder tables for text/audio/video until extraction pipelines are connected. \n",
    "- Class imbalance persists; monitor PR curves and calibration in later notebooks. \n",
    "- Alignment between modalities may vary by session; verify IDs and time boundaries upstream.\n",
    "\n",
    "**Reproducibility**\n",
    "- Python 3.11 (`venv`) \n",
    "- Core libs: `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `z3-solver` \n",
    "- Optional libs: `sentence-transformers`, `librosa`/`opensmile`, `openface` (CLI) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Closing summary & next steps\n",
    "\n",
    "- Engineered tabular features (PHQ 8 with standardized interactions). \n",
    "- Added SMT guardrails to catch data/timing/split issues early. \n",
    "- Assembled a merged, subject-disjoint multimodal dataset ready for modeling.\n",
    "\n",
    "**Next:** Notebook 04 - Multimodal Modeling & Fusion (LR/RF/baseline NN; late fusion vs. early fusion; calibration & interpretability).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trauma_ai)",
   "language": "python",
   "name": "trauma_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
